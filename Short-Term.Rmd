---
title: "Title"
author: "Stephen Synchronicity"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
output: 
  html_document: 
    self_contained: yes
    css: C:\Users\Administrator\Documents\R\win-library\3.5\neuhwk\rmarkdown\templates\DA5030\resources\bootstrap.min.css
    highlight: zenburn
    keep_md: no
    theme: spacelab
    toc: TRUE
    toc_float: TRUE
    df_print: paged
    code_folding: hide
---
```{r setup, include=FALSE}
# Knitr Options
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE, cache = TRUE, fig.align = 'center', fig.height = 5, fig.width = 7.5, tidy = TRUE, tidy.opts = list(width.cutoff = 80))
options(scipen = 12)
# Make reproducible
set.seed(1)
# Load packages

source("~/R/Quant/JobsScripts/parameters.R")
HDA::startPkgs(c("dplyr","magrittr","printr", "AlpacaforR", "qf"))
purrr::walk(list.files("db", full.names = T, pattern = "\\.db$"), ~{file.copy(qf::wd(.x), qf::wd(fs::path("db", "backup", basename(.x))), overwrite = TRUE)})
# # Attach dependencies
# rmarkdown::html_dependency_jquery()
# rmarkdown::html_dependency_bootstrap("spacelab")
# rmarkdown::html_dependency_jqueryui()
# # Use Chunk Titles to add Bold Headi1ngs to Chunks
# source("~/R/Scripts/addChunkTitles.R")
# rmd <- addChunkTitles(rstudioapi::getSourceEditorContext()$path)
# write(rmd,file=(rstudioapi::getSourceEditorContext()$path))
# # HTML to Latex
# source("~/R/Scripts/HTMLtoLatex.R")
# rmd <- HTMLtoLatex(rstudioapi::getSourceEditorContext()$path)
# write(rmd,file=(rstudioapi::getSourceEditorContext()$path))

```


```{r 'Get Positions', eval = F}
.token <- gargle::oauth_app_from_json("~/R/google_oauth.json")
googlesheets4::gs4_auth("sholsen@alumni.emory.edu", token = .token)
Positions <- googlesheets4::range_read("https://docs.google.com/spreadsheets/d/1Iazn6lYRMhe-jdJ3P_VhLjG9M9vNWqV-riBmpvBBseg/edit#gid=0", sheet = "Personal")
(Positions_v <- Positions[-c(1, 2), 1, drop = T])
```

```{r 'Get Cruelty Free Investing table', eval = F}
.htm <- xml2::read_html("http://crueltyfreeinvesting.org/")
`%>%` <- dplyr::`%>%`
cfi <- dplyr::bind_rows(.htm %>% 
  rvest::html_node("div.cfi_left_comp_list") %>% 
  rvest::html_nodes("article") %>% 
  purrr::map_dfr(~{
    .n <- rvest::html_node(.x, "div.company-symbol") %>% rvest::html_node("a")
    tibble::tibble(
      symbol = rvest::html_text(.n),
      full_name = rvest::html_node(.x, "h2.entry-title") %>% rvest::html_text(),
      cruelty_free = FALSE,
      reason = rvest::html_attr((.n), "alt")
      )
    }),
  .htm %>% 
    rvest::html_node("div.cfi_comp_right") %>% 
    rvest::html_nodes("article") %>% 
  purrr::map_dfr(~{
    .n <- rvest::html_node(.x, "div.company-symbol") %>% rvest::html_node("a")
    tibble::tibble(
      symbol = rvest::html_text(.n),
      full_name = rvest::html_node(.x, "h2.entry-title") %>% rvest::html_text(),
      cruelty_free = TRUE,
      reason = rvest::html_attr((.n), "alt")
      )
    })
)
readr::write_csv(cfi, glue::glue("~/R/Quant/db/cruelty_free_symbols{Sys.Date()}.csv"))
```
```{r 'read cfi'}
cfi <- readr::read_csv(glue::glue("~/R/Quant/db/cruelty_free_symbols{Sys.Date()}.csv"))
```

```{r 'Filter for CF'}
library(magrittr)
(cruel <- {Positions_v %in% (cfi %>% filter(cruelty_free == FALSE) %>% dplyr::pull(symbol))} %>% 
  {Positions_v[.]})
# View reasons
cfi[cfi$symbol %in% cruel, c("full_name","reason")]
# Filter for tradable assets
to_add <- AlpacaforR::assets(Positions_v[!Positions_v %in% cruel]) %>% 
  filter(tradable)
# update watchlist
wl <- AlpacaforR::watchlist("Primary")
  
wl <- AlpacaforR::watchlist("Primary", symbols = to_add$symbol[!to_add$symbol %in% wl$symbol], action = "a")
  
Positions_v <- wl$symbol %>% stats::setNames(nm = .)
```

```{r, 'Retrieve RV lagged data', eval = F}
# for RVs to be computed properly, the data must start from the maximum duration  RV windows prior.
market_data_rvs <- function(x, multiplier, timeframe, full = TRUE, max.duration = lubridate::days(200), begin) {
  if (is.character(x)) .x <- x
  else
    .x <- get_sym(x)
  
  if (missing(multiplier) && missing(timeframe) && is.data.frame(x))
    .time_interval <- time_int(x)
  
  from <- from - 
  dat <- AlpacaforR::market_data(ticker, v = 2, multiplier = multiplier, timeframe = timeframe, from = from, to = to, full = full)
}
# Get lagged data
lagdat <- market_data_rvs(Positions_v, timeframe = "h", from = "2020-03-01", to = dat_hr[[1]]$time[1], full = TRUE)
# fix LVGO
lagdat$LVGO <- AlpacaforR::market_data("LVGO", v = 2, timeframe = "h", from = lubridate::mdy("07/26/2019"), full = T)[[1]]
#impute missing
lagdat <- purrr::map(lagdat, ~{
  if (inherits(.x, "data.frame")) {
    .out <- mice::complete(mice::mice(as.data.frame(.x), m = 1, threshold = 1, method = c("midastouch", "cart", rep("rf", 4), "", "rf")))
  }
      
  })
dat_hr <- purrr::pmap(list(lagdat, dat_hr, names(lagdat)), ~{
  .ti <-params$time_index(.x)
  .out <- dplyr::distinct_at(dplyr::bind_rows(
  tibble::as_tibble(.x),
  tibble::as_tibble(.y)
), .ti, .keep_all = TRUE)
  .in <- dplyr::filter(.out, time != .y[[.ti]])
  if (nrow(.in) > 0) {
    RSQLite::dbWriteTable(db, ..3, .in, append = TRUE)
  }
  .ti <- rlang::sym(.ti)
  .out <- tsibble::as_tsibble(dplyr::filter(.out, lubridate::hour(!!.ti) >= 10 & lubridate::hour(!!.ti) < 17), index = !!.ti, regular = FALSE)
  attr(.out, "Sym") <- ..3
  .out
})
```


```{r, 'Write data to db', eval = F}
db <- RSQLite::dbConnect(RSQLite::SQLite(), "db/hours.db")
purrr::iwalk(dat, ~{
  RSQLite::dbWriteTable(db, paste0(.y), .x, overwrite = T)
})
RSQLite::dbListTables(db)
RSQLite::dbDisconnect(db)
```

## Retrieve Data from DB
```{r 'Set Training dates'}
.date <- list(
  begin = lubridate::ymd_hm("2021-01-20 09:00", tz = "America/New_York"),
  end = lubridate::now()
)
```

```{r 'Retrieve daily data from db'}
dat_da <- qf::get_data("raw", db = "days")
```

```{r 'backfill data', eval = FALSE}
# Method using 200 day period (doesn't work due to limitations of TTR package to only use numeric lag for indexes and does not handle actual time periods)
# cal <- AlpacaforR::calendar("2020-01-01")
# cal <- expand_calendar(cal, timeframe = "hour", multiplier = 1)
# .begin <- cal %>% dplyr::pull(d) %>% unique %>% {.[findInterval(as.Date(.date$begin), .) - 200]} 
sma.wind <- qf::time_windows(dat[[1]], c(params$wind$multipliers, sma = 200))
# The beginning should be the training date beginning index - the maximum index lag for the 200day SMA
.begin <- time_index(dat_hr[[1]], "v") %>% .[findInterval(.date$begin, .) - max(sma.wind)]
dat_hr <- qf::get_data("raw", positions = qf::Positions_v, update = TRUE, begin = .begin)
```

```{r 'Retrieve hourly data from db and update'}
# [which(names(Positions_v) == "EMNT"):length(Positions_v)]
dat_hr <- qf::get_data("raw", positions = qf::Positions_v)

```

## Add Variables

```{r 'Join Hourly Data and RVs'}
dat_rv <- get_data("rv", positions = qf::Positions_v)
```
```{r, "Read/Write/Remove RVs", eval = F}
purrr::walk(Positions_v,~{
  #.out <- RSQLite::dbReadTable(db, paste0(.x, "3_rv"))
  #RSQLite::dbRemoveTable(db, paste0(.x, "2_rv"))
  #RSQLite::dbRemoveTable(db, paste0(.x, "3_rv"))
  #RSQLite::dbWriteTable(db, paste0(.x, "_rv"), as.data.frame(.out), overwrite = T)
})
```

### Response Variables: Trailing Stop Tuning
```{r 'Add RVs to Data'}
# Determine the first index for which a sell point did not compute
.start_ind <- purrr::map(dat_rv, ~{
  .d <- .x
  .d[[qf::time_index(.d)]][purrr::map_dbl(.d[stringr::str_detect(names(.d), "_ind")], ~min(which(is.na(.x)))) %>% 
    min]
  
})
# filter for the first index for which a sell point did not compute

dat <- purrr::map2(dat_hr, .start_ind, ~{
  .t <- qf::time_index(.x, "l")
  if (is.null(.y)) .y <- min(.x[[.t]])
  dplyr::filter(.x, !!.t >= .y)
})
.add <- T
save(dat, .add, file = "data/input_rv.Rdata")
rstudioapi::jobRunScript("~/R/Quant/JobsScripts/AddRVstoData.R", exportEnv = "R_GlobalEnv", importEnv = F, workingDir = getwd())
# Save
```

```{r 'Retrieve RV Data'}
dat_rv <- qf::get_data("rv", AlpacaforR::watchlist("Primary")$symbol)
```


### Response Variables: Calculating Returns
```{r 'Map Returns', eval = F}
dat <- dat_rv %>% purrr::map( ~{dplyr::filter(.x, time > .date$begin & time <= .date$end)})

save(dat, file = "data/input_mapret.Rdata")
rstudioapi::jobRunScript(path = "JobsScripts/MapReturns.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv")
```

```{r 'Determine best TSL', eval = F}
load(file = "data/output_mapret.Rdata")
best_tsl <- qf::best_returns(ret)
db <- qf::get_data("db")
RSQLite::dbWriteTable(db, "best_tsl", best_tsl, append = TRUE)
RSQLite::dbDisconnect(db)
```
```{r 'Process TSL Percent Thresholds', eval = F}
tsl_pct <- tibble::rownames_to_column(purrr::imap_dfc(purrr::map_depth(ret.p, 2, ~table(.x$p) %>% {as.numeric(names(.[which.max(.)]))}), ~{
  .x <- purrr::map_if(.x, ~length(.x) == 0, ~NA)
  stats::setNames(as.data.frame(unlist(.x)), .y)
  }))
db <- qf::get_data("db")
RSQLite::dbWriteTable(db, "tsl_pct", tsl_pct, overwrite = TRUE)
RSQLite::dbDisconnect(db)
```

## Independent Variables

```{r "oot dates"}
.oot <- list(
  begin = lubridate::ymd_hm("2021-01-20 09:00", tz = "America/New_York"),
  end = Sys.Date()
)
```

```{r 'Feature Addition'}
max_wind <- qf::time_windows(dat_hr[[1]], c(params$wind$multipliers, sma = 200))
dat <- dat_hr %>% purrr::map(cut_iv, begin = .oot$begin, max_wind = max(max_wind) * 2)
save(dat, file = "~/R/Quant/data/input_AddIVstoData.Rdata")
rstudioapi::jobRunScript("~/R/Quant/JobsScripts/AddIVstoData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
```

```{r 'Retrieve Data with IVs'}
dat_iv <- qf::get_data("iv", Positions_v)
```


# Modeling
### Retrieve Data
```{r 'Pull Complete Dataset from DB'}
dat_full <- qf::get_data("full", positions = AlpacaforR::watchlist("Primary")$symbol)
```

## Hyperparameter Tuning
```{r 'Tune Hyperparameters'}
dat <- dat_full
# Whether to save tuning grids. These take a long time to build and may be lost to an error before the function completes unless saved.
save_intermediate = TRUE
save(dat, file = "~/R/Quant/data/input_TuneHP.Rdata")
rstudioapi::jobRunScript("~/R/Quant/JobsScripts/TuneHP.R", exportEnv = "R_GlobalEnv", importEnv = F, workingDir = getwd())
```

### Retrieve the best tuning parameters
```{r 'Extract Best Tuning Params'}
db <- qf::get_data("db")
.db_tbls <- RSQLite::dbListTables(db)
.mod_syms <- stats::na.exclude(stringr::str_subset(.db_tbls, "tune$")) %>% 
  stringr::str_extract("^[A-Z]+") %>% 
  unique %>% 
  stats::setNames(nm = .)
# get the tuning parameters
.tunes <- purrr::map(.mod_syms, ~{
  .sym <- .x
  .tunes <- stats::na.exclude(stringr::str_subset(stringr::str_subset(.db_tbls, "tune$"), .x))
  names(.tunes) <- stringr::str_extract(.tunes, "[a-z]+(?=\\.tune$)")
  purrr::imap(.tunes, ~{
    .tune <- RSQLite::dbReadTable(db, .x)
    # get the msd table for converting RMSE values
    .msd <- RSQLite::dbReadTable(db, glue::glue("{.sym}_msd"))
    #assuming one RV is present, retrieve it's standard deviation
    .sd <- .msd[stringr::str_which(.msd$nm, "rv$"), "sd", drop = TRUE]
    .tune <- .tune %>%
      dplyr::mutate_at(dplyr::vars(mean, std_err), ~ . * .sd) %>% 
      dplyr::mutate_at(dplyr::vars(time), lubridate::as_datetime, tz = "America/New_York") %>%
      dplyr::arrange(mean) %>% 
      utils::head(3)
  })
})
# bind the parameters to the datasets
dat_trained <- purrr::map2(dat_full[names(.mod_syms)], .tunes, ~{
  .d <- .x
  .tune <- purrr::map(.y, ~{
    .x[,1:(which(names(.x) %in% ".metric") - 1)]
  })
  data.table::setattr(.d, "tune", .tune)
})
RSQLite::dbDisconnect(db)
```

## Testing on Out-of-time (OOT) data

```{r 'Create OOT Data'}
# TODO update db first
dat_4_oot <- purrr::pmap(list(dat_trained, dat_full[names(dat_trained)], names(dat_trained)), ~{
  .tid <- params$time_index(.x)
  # determine the amount of tail
  .tail <- nrow(.y) - which(.y[[.tid]] %in% utils::tail(.x[[.tid]], 1)) + 200
  utils::tail(.y, .tail) %>% 
    data.table::setattr("Sym", ..3)
  
})

# TODO Build new RVs and IVs

.tid <- params$time_index(dat_full[[1]])
dat_oot <- purrr::pmap(list(dat_full[.mod_syms], .mod_syms, dat_trained), ~{
  .d <- .x
  .sym <- .y
  .tune <- attr(..3, "tune")
  .out = list(
    train = dplyr::filter(.x, !!rlang::sym(.tid) <= .oot$end & !!rlang::sym(.tid) >= .oot$begin),
    test = dplyr::filter(.x, !!rlang::sym(.tid) > .oot$end)
  )
  .out <- purrr::map(.out, ~{
    .x %>%
      data.table::setattr("tsl", attr(.d, "tsl")) %>% 
      data.table::setattr("Sym", .sym) %>% 
      data.table::setattr("tune", .tune)
  })
})
```

### Final Models, Predictions and Performance
```{r 'Make final models from tuned hp, predict and assess performance'}
save(dat_oot, file = "data/input_FitTunedHP.Rdata")
rstudioapi::jobRunScript("~/R/Quant/JobsScripts/FitTunedHP.R", exportEnv = "R_GlobalEnv", importEnv = F)
```

```{r 'Probabilities and Performance'}
.perf <- purrr::map_depth(preds, 3, ~{
    out <- list(
    prob = sum(purrr::map_dbl(.x$performance$returns, ~{
      utils::tail(.x, 1)$pct_returns_c
      }) > .x$performance$buy_hold) / length(.x$performance$returns),
    prof = mean(purrr::map_dbl(.x$performance$returns, ~{utils::tail(.x, 1)$pct_returns_c})) - .x$performance$buy_hold
    )
    
})
```


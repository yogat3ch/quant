---
title: "Stock Tools"
author: "Stephen Synchronicity"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
output: 
  html_document: 
    self_contained: yes
    css: C:\Users\Stephen\Documents\R\win-library\3.4\neuhwk\rmarkdown\templates\report\resources\bootstrap.min.css
    highlight: zenburn
    keep_md: no
    theme: spacelab
    toc: yes
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = T,warning=FALSE,cache=TRUE, fig.align='center', fig.height=3.5, fig.width=5, tidy=TRUE, tidy.opts=list(width.cutoff=80))
# rmarkdown::html_dependency_jquery()
# rmarkdown::html_dependency_bootstrap("spacelab")
# rmarkdown::html_dependency_jqueryui()
set.seed(1)
options(scipen=12)
HDA::startPkgs(c("tidyverse","magrittr","lubridate", "AlpacaforR"))
# ----------------------- Fri Jul 19 09:03:50 2019 ------------------------#
#Functions
source("JobsScripts/parameters.R")
```

# Objectives
<ol>
  <li>To develop a tax sensitive patterned trading algorithm that does not exceed a tax bracket for a given year.</li>
  <li>To generate at least 10% annual returns.</li>
  <li>To develop a workflow that rapidly transforms OHLC into an indicator rich xts or tbl_time</li>
  <li>To outperform the following benchmark strategies on any single position:<ul>
    <li>The Gold Cross (mean reversion)</li>
    <li>The 12,26,9 MACD</li>
    <li>The 14 RSI</li>
    <li>Buy & Hold</li>
  </ul> </li>
  <li>To optimize the portfolio as a whole according to the following three ratios:<ul>
    <li>Sharpe Ratio $\frac{\text{mean return}}{\text{volatility}}$</li>
    <li>Information Ratio $\frac{\text{Annualized Return}{\text{Annualized Risk}}$</li>
    <li>Profit factor: $\frac{\text{Gross Profit}}{\text{Gross Losses}}$ </li>
  </ul> </li>
</ol>

TODO - 2019-05-30 1903
- Unbundle Add TSLs from Add Vars DONE
- Standardize jobDat_in, jobDat_out to pass data to jobs DONE
- Convert Add RVs to parallel DONE
- Convert optimReturns to parallel UNTESTED
- Rework ML model builder to build for the optimum TSL only
- Weighted TSLs by recency?
```{r 'Load Data'}
# ----------------------- Sat Mar 24 08:17:41 2018 ------------------------#
# Inputs: Timeseries data, Lag for LM
# Outputs: Coefficient - Derivative
# Use ADX http://www.stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_directional_index_adx
# Use Parabolic SAR http://www.stockcharts.com/school/doku.php?id=chart_school:technical_indicators:parabolic_sar
# Use RSI http://www.stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi
# Create Neural net signal tracker
# ----------------------- Mon Aug 06 16:36:13 2018 ------------------------#
# Load my personal portfolio and watch list

.token <- gargle::oauth_app_from_json("~/R/google_oauth.json")
googlesheets4::sheets_auth("sholsen@alumni.emory.edu", token = .token)
Positions <- googlesheets4::sheets_read("https://docs.google.com/spreadsheets/d/1Iazn6lYRMhe-jdJ3P_VhLjG9M9vNWqV-riBmpvBBseg/edit#gid=0", sheet = "Personal")
# ----------------------- Tue Jun 26 13:52:59 2018 ------------------------#
# Import Dad's portfolio
#Dad.positions <- googlesheets::gs_read(gsPositions,ws=2)
```
```{r 'Get Symbols'}
(Positions_v <- Positions[-c(1, 2), 1, drop = T])
#Dad.positions_v <- Dad.positions[-c(1),1,drop=T]
```


<a href="https://iextrading.com/developer/docs/#chart" target="_blank">IEX Documentation: Charts</a>
```{r 'IEX Functions and Chart Fields'}
iex_fields <- tibble::tribble(
  ~field, ~type, ~desc,
               "high",  "number",                                                                                                 "is available on all charts.",
               "low", "number",                                                                                                "is available on all charts.",
            "volume", "number",                                                                                                "is available on all charts.",
             "label", "number",  "is available on all charts. A variable formatted version of the date depending on the range. Optional convienience field.",
    "changeOverTime", "number", "is available on all charts. Percent change of each interval relative to first value. Useful for comparing multiple stocks.",
              "date", "string",                                                                                                "is available on all charts.",
              "open", "number",                                                                                                "is available on all charts.",
             "close", "number",                                                                                                "is available on all charts.",
  "unadjustedVolume", "number",                                                                                              "is not available on 1d chart.",
            "change", "number",                                                                                              "is not available on 1d chart.",
     "changePercent", "number",                                                                                              "is not available on 1d chart.",
              "vwap", "number",                                                                                              "is not available on 1d chart."
  )

```


## Financials
```{r 'Responses per Query Type for Filtering Purposes'}
# ----------------------- Fri Jun 29 18:52:46 2018 ------------------------#
# The IEX API has similarily formatted API calls of the following types:
# stats, quote, earnings, financials
# Each of the types has specific response variables that can be filtered - these are found in the table below.

library("rvest")
iex_responses <- list()
iex_responses$stats <- rvest::html_table(xml2::read_html("https://iextrading.com/developer/docs/#key-stats") %>% rvest::html_node(xpath = "/html/body/div[2]/div[2]/table[18]"))
iex_responses$quote <- rvest::html_table(xml2::read_html("https://iextrading.com/developer/docs/#key-stats") %>% rvest::html_node(xpath = "/html/body/div[2]/div[2]/table[27]"))
iex_responses$earnings <- rvest::html_table(xml2::read_html("https://iextrading.com/developer/docs/#key-stats") %>% rvest::html_node(xpath = "/html/body/div[2]/div[2]/table[11]"))
iex_responses$financials <- rvest::html_table(xml2::read_html("https://iextrading.com/developer/docs/#key-stats") %>% rvest::html_node(xpath = "/html/body/div[2]/div[2]/table[13]"))
```

```{r 'Financial Stats'}
# ----------------------- Wed Aug 08 17:13:33 2018 ------------------------#
# Create a Data frame of financial metrics using IEX

quotes <- iex_stock_req(tSymbols = Positions_v,type = "quote", query_filter = c("marketCap","peRatio","week52High","week52Low","ytdChange"))
stats <- iex_stock_req(tSymbols = Positions_v,type = "stats", query_filter = c("peRatioHigh","peRatioLow","week52change	","returnOnEquity","consensusEPS","ttmEPS","profitMargin","year5ChangePercent","year1ChangePercent","ytdChangePercent","month6ChangePercent","month3ChangePercent","month1ChangePercent","cash","debt"))
fin_stats <- cbind(quotes,stats)

```

```{r 'Batch Symbol Timeseries Data Request', fig.dim=c(8,12)}
#Positions_ts <- lapply(Positions_v, function(x)Riex::iex.chart(x, "5y", iex_sk = keyring::key_get("IEX")))
Positions_ts <- iex_batch_req(tSymbols = Positions_v, range = "5y", filter = iex_fields[c(1:3,6:8,10:11),1,drop=T])
#Dad.positions_ts <- iex_batch_req(tSymbols = Dad.positions_v, range = "1y", filter = iex_fields[c(1:3,6:8,10:11),1,drop=T]) 
dat$WORK %>% dplyr::mutate_at(dplyr::vars(time), ~ lubridate::as_date(.)) %>% dplyr::mutate(Q = lubridate::quarter(time, with_year = T)) %>% dplyr::group_by(Q) %>% dplyr::summarize(high = mean(high)) %>%   ggplot2::ggplot( ggplot2::aes(time, high)) + ggplot2::geom_bar(stat = "identity", color = "steelblue3") + ggplot2::scale_x_date(date_breaks = "3 months", date_labels = "%m/%y")+ggplot2::ylab("High")

data001 %>%
  dplyr::mutate(Q = lubridate::quarter(date001, with_year = T)) %>% # Add a column with the quarter
  dplyr::group_by(Q) %>% # group by that column
  dplyr::summarize(Antibiotic_by_Q = mean(Antibiotic.Total)) %>% # summarize by quarter by taking the mean
  ggplot2::ggplot(ggplot2::aes(Q, Antibiotic_by_Q)) + ggplot2::geom_bar(stat = "identity", fill = "steelblue3") + ggplot2::ylab("Antibiotic Total (Grams)") # Graph by the new variables, x is no longer a date so remove it
```
```{r 'Update Data from Alpaca'}
get_positions <- function (ticker = NULL, live = FALSE) 
{
  url = AlpacaforR::get_url(live)
  headers = AlpacaforR::get_headers(live)
  positions = httr::GET(url = paste0(url, "/v1/positions"), 
    headers)
  positions = AlpacaforR::response_text_clean(positions)
  if (length(positions) == 0) 
    cat("No positions are open at this time.")
  else if (is.null(ticker)) {
    positions[, c(5:6, 8:ncol(positions))] %<>% purrr::map_dfc(as.numeric)
    return(positions)
  }
  else {
    positions[, c(5:6, 8:ncol(positions))] %<>% purrr::map_dfc(as.numeric)
    positions <- subset(positions, symbol == ticker)
    return(positions)
  }
}
ticker <- r$symbol

Positions_ts <- AlpacaforR::get_bars(Positions_v, from = lubridate::today() - lubridate::weeks(4), timeframe = "5m")
Positions_ts %<>% purrr::compact() 
```
```{r 'Add Additional data for IVs'}
source("JobsScripts/parameters.R")
# ----------------------- Fri Jul 19 09:36:31 2019 ------------------------#
#Add Additional data to account for NA on indicators
purrr::map(Positions_ts,~{
  if (xts::is.xts(.x)) {
     earliest.date <- min(stats::time(.x))
  } else {
     td_nm <- stringr::str_extract(names(.x), "^time$|^date$") %>% subset(subset = !is.na(.)) %>% .[1]
     earliest.date <- min(.x[[td_nm]])
  }
})
Positions_ext <- purrr::imap(Positions_ts, ed = earliest.date, wind = params$wind, function(.x, .y, ed, wind){
  td_nm <- stringr::str_extract(names(.x), "^time$|^date$") %>% subset(subset = !is.na(.)) %>% .[1]
  if (xts::is.xts(.x)) {
   to <- min(stats::time(.x)) - lubridate::days(1)
   from <- min(stats::time(.x)) - {wind[[4]] * 4}
 } else {
   
   to <- min(.x[[td_nm]]) - lubridate::days(1)
   from <- to - {wind[[4]] * 4}
 }
  if (!to < ed) {
    message(paste0("No data to retrieve for:", .y))
    return(.x)
  }
  message(paste0("Retrieving data for: ",.y))
  .data <- AlpacaforR::get_bars(.y, from = from, to = to)
  if (xts::is.xts(.x)) {
    .data <- xts::xts(.data[[1]][, - c("time")], order.by = .data[[1]][["time"]])
    out <- xts::rbind.xts(.data, .x)
  } else {
      out <- rbind.data.frame(.data[[1]], .x)
      out <- tibbletime::tbl_time(out, index = !!td_nm)
  }
  attr(out, "Sym") <- .y
  return(out)
})

```

 
```{r 'Add Sign indicator_Change Date'}
# ----------------------- Tue Jun 26 14:54:21 2018 ------------------------#
# Add sign indicator for color change in candlestick graph 

Positions_ts %<>% lapply(FUN=function(l){
  l$chg <- sign(as.numeric(l$changePercent)) %>% as.character() %>% generics::as.factor()
  l})
# Dad.positions_ts %<>% lapply(FUN=function(l){
#   l$chg <- sign(as.numeric(l$changePercent)) %>% as.character() %>% as.factor()
#   l})

```
```{r 'Rename to Upper' , eval = F}
Positions_ts %<>% lapply(function(l){
  names(l) %<>% stringr::str_to_title() 
  return(l)
})
```
```{r 'Rename to lower'}
Positions_ts %<>% lapply(function(l){
  names(l) %<>% tolower() 
  return(l)
})
```
```{r 'Fill Dates for Proper Windowing'}
Positions_ts %<>% purrr::imap(function(.x, .y){ # Actual Time for measuring exact weeks, quarters and approx. months
  if (xts::is.xts(.x)){
out <- xts::xts(order.by = seq(min(stats::time(.x)), max(stats::time(.x)), by = "1 days")) %>% xts::merge.xts(l) %>% zoo::na.locf()
xts::xtsAttributes(out) <- list(Sym = .y)
  } else {
    td_nm <- stringr::str_extract(names(.x), "^time$|^date$") %>% .[1]
    out <- data.frame(time = seq(min(.x[, td_nm, drop = T]), max(.x[, td_nm, drop = T]), by = "1 days"), stringsAsFactors = F) %>% dplyr::left_join(.x) %>% na.locf 
  out <- tibbletime::tbl_time(out, index = !!td_nm)
  attr(out, "Sym") <- .y
}
return(out)
})
.x <- Positions_ts[[1]]
```

```{r 'Add Risk Metrics'}
Positions.scaled_ts <- lapply(Positions_ts,FUN = function(l){l$close %<>% scale %>% as.numeric
  l})
key_stats <- lapply(seq_along(Positions.scaled_ts), pos=Positions.scaled_ts,timeframe = "year", FUN = function(l,pos,timeframe){
if(!is.null(timeframe) & any(class(pos[[1]])=="data.frame")){
  data <- pos[[l]] %>% dplyr::filter(date > lubridate::floor_date(lubridate::today(), unit = timeframe))
}else if(!is.null(timeframe) & any(class(pos[[1]])=="xts")){
  data <- pos[[l]][stats::time(pos[[l]]) > lubridate::floor_date(lubridate::today(),timeframe),]
}else {
  data <- pos[[l]]
}
data$ind <- 1:nrow(data)
mod <- stats::lm(close ~ ind, data = data)
out <- vector("numeric",length = 4)
out[1] <- mod[["coefficients"]][["ind"]]
out[2] <- caret::RMSE(mod[["fitted.values"]],mod[["model"]][["close"]])
mod_sum <- summary(mod)
out[3] <- mod_sum %>% {.[["adj.r.squared"]]}
out[4] <- sum(sign(mod_sum[["residuals"]]) < 0)/length(mod_sum[["residuals"]])
out[5] <- (mod_sum[["sigma"]] + mean(abs(mod_sum[["residuals"]][mod_sum[["residuals"]] < 0]),na.rm = T)) / pos[[l]][nrow(pos[[l]]),"open"]
names(out) <- c("coef","rmse","adj.r","pctNeg","sigma")
return(out)
})
names(key_stats) <- names(Positions.scaled_ts)
key_stats %<>% do.call("rbind",.) # From list to dataframe
```

```{r 'Combine & Financial Stats'}
fin_stats_complete <- cbind(key_stats,fin_stats) %>% as.data.frame
DT::datatable(fin_stats_complete)
```

## Statistical Signficance Indicator
```{r 'Two Sigma Indicator'}
# Needs 95% significance uptrend from 200day SMA
sig <- function(.,v){
  sapply(.,v=.,FUN=function(.,v)stats::pnorm(.,mean(v),sd=stats::sd(v)),simplify = "array")
}

dplyr::map_lgl(seq_along(Dad.positions_ts),pos = Dad.positions_ts,.f=function(l,pos){
  zoo::rollapply()
thr <- stats::qnorm(1.1,mean(pos[[l]][,"open",]),stats::sd(pos[[l]][,"open"]))
any(pos[[l]][,"open"] > thr)
  })
```



## Candlestick Graphs
### SEP IRA 
```{r 'Charts for SEP IRA'}
# ----------------------- Tue Jun 26 14:15:19 2018 ------------------------#
# Try with ggplot2 & plotly
# ----------------------- Mon Nov 19 08:54:35 2018 ------------------------#
# BUG(arguments imply differing number of rows: 251, 0) 

Dad.positions_df <- lapply(Dad.positions_ts,function(l){
out <- as.data.frame(l)
out$date <- row.names(out) %>% lubridate::ymd()
return(out)
})
dad_pos_ggp <- lapply(seq_along(Dad.positions_ts),pos = Dad.positions_ts,FUN=function(l,pos){
  pos[[l]] %>% 
  ggplot2::ggplot(data= ., ggplot2::aes(x=date, colour = chg)) +
  ggplot2::theme_bw() +
  ggplot2::geom_linerange(ggplot2::aes(ymin=low, ymax=high)) +
  ggplot2::geom_segment(ggplot2::aes(y = open, yend = open, xend = date - {as.numeric(xts::periodicity(date)[1])} / 2 )) +
  ggplot2::geom_segment(ggplot2::aes(y = close, yend = close, xend = date + {as.numeric(xts::periodicity(date)[1])} / 2)) +
  ggplot2::geom_smooth(method = "lm",ggplot2::aes(y = close))+
  #geom_text(aes(x = lmeq_x, y = lmeq_y, label = lm_eqn(l)), parse = TRUE) +
  ggplot2::scale_colour_manual(values = c("-1" = "darkred", "1" = "darkgreen", "0" = "gray")) + ggplot2::theme(legend.position='none') + 
    ggplot2::labs(title = names(pos)[l],
    subtitle = "1 Year Historical",
    caption = "",
    x = "Date",y = "Closing Price") +
    ggplot2::theme(plot.title = ggplot2::element_text(hjust = .5),plot.subtitle = ggplot2::element_text(hjust = .5))
    
})

lapply(dad_pos_ggp,FUN=function(ggp){plotly::ggplotly(ggp, dynamicTicks = T, tooltip = c("y","high","low"))}
)
```

### Individual
```{r 'Relationship between RV and closing Price'}
 
purrr::map(stringr::str_extract_all(names(Positions_ts[[1]]), ".*rv$") %>% subset(subset = nchar(.) > 0) %>% purrr::compact(), .data = Positions_ts[[1]], function(.x, .data){
  
  v <- rlang::sym(.x)
  .data %<>% dplyr::mutate(y_var = !!v * close)
  .data %>% ggplot2::ggplot(data = .,mapping = ggplot2::aes(x = time))+
ggplot2::geom_line(ggplot2::aes(y = y_var))+
  ggplot2::geom_line(ggplot2::aes(y = close)) + 
    ggplot2::xlab(.x)+
    ggplot2::ylab("")+
    ggplot2::ggtitle(paste0("Correlation: ",stats::cor(.data[["y_var"]], .data[["close"]])))+
    ggplot2::theme(plot.title = ggplot2::element_text(hjust = .5),plot.subtitle = ggplot2::element_text(hjust = .5))
})
```

```{r 'Charts for Individual Portfolio'}
pos_ggp <- lapply(seq_along(pos),pos = pos,FUN=function(l,pos){
  pos[[l]] %>% 
  ggplot2::ggplot(data= ., ggplot2::aes(x=date, colour = chg)) +
  ggplot2::theme_bw() +
  ggplot2::geom_linerange(ggplot2::aes(ymin=low, ymax=high)) +
  ggplot2::geom_segment(ggplot2::aes(y = open, yend = open, xend = date - {as.numeric(xts::periodicity(date)[1])} / 2 )) +
  ggplot2::geom_segment(ggplot2::aes(y = close, yend = close, xend = date + {as.numeric(xts::periodicity(date)[1])} / 2)) +
  ggplot2::geom_smooth(method = "lm",ggplot2::aes(y = close))+
  #geom_text(aes(x = lmeq_x, y = lmeq_y, label = lm_eqn(l)), parse = TRUE) +
  ggplot2::scale_colour_manual(values = c("-1" = "darkred", "1" = "darkgreen", "0" = "gray")) + ggplot2::theme(legend.position='none') + 
    ggplot2::labs(title = names(pos)[l],
    subtitle = "1 Year Historical",
    caption = "",
    x = "Date",y = "Closing Price") +
    ggplot2::theme(plot.title = ggplot2::element_text(hjust = .5),plot.subtitle = ggplot2::element_text(hjust = .5))
    
})

lapply(pos_ggp,FUN=function(ggp){plotly::ggplotly(ggp, dynamicTicks = T, tooltip = c("y","high","low"))}
)
```





# Signals & Indicators

Set the windows:
```{r 'Set Windows'}
wind  <-  c(weeks = 7, months = 7*4, quarters = 7*4*3)
```
<button class="btn btn-sm btn-success" href="#sma" role="button" data-toggle="collapse" data-target="#demo">View Simple Moving Averages</button>
<div id="sma" class="collapse">
```{r 'Add SMA'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add Simple Moving Averages") 
```
</div>

### Add ADX
<button class="btn btn-sm btn-success" href="#ADX" role="button" data-toggle="collapse" data-target="#demo">View ADX</button>
<div id="adx" class="collapse">
```{r 'Add ADX'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add ADX") 
```


<strong>Uptrend</strong>
[type = stock] AND [country = US] 
AND [Daily SMA(20,Daily Volume) > 100000] 
AND [Daily SMA(60,Daily Close) > 10] 

AND [Daily ADX Line(14) > 20] 
AND [Daily Plus DI(14) crosses Daily Minus DI(14)] 
AND [Daily Close > Daily SMA(50,Daily Close)]

<strong>Downtrend</strong>
[type = stock] AND [country = US] 
AND [Daily SMA(20,Daily Volume) > 100000] 
AND [Daily SMA(60,Daily Close) > 10] 

AND [Daily ADX Line(14) > 20] 
AND [Daily Minus DI(14) crosses Daily Plus DI(14)] 
AND [Daily Close < Daily SMA(50,Daily Close)]

```{r 'Add ADX Signal'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add ADX signal") 
```

```{r 'Add Compressed ADX'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add Compressed ADX") 
```
</div>


## Add Variables in Article
Referencing: <a href="https://www.quantinsti.com/blog/predictive-modeling-algorithmic-trading/">R Predictive Algorithm</a>
### Add Williams %R
<button class="btn btn-sm btn-success" href="#wpr" role="button" data-toggle="collapse" data-target="#demo">View Williams %R</button>
<div id="wpr" class="collapse">
```{r 'Add Williams Indicator'}
# ----------------------- Fri May 04 10:35:38 2018 ------------------------#
# Add the Williams %R where per = vector of period lengths.
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add Williams Percent R") 
```
</div>

### Add Relative Strength Indicator
<button class="btn btn-sm btn-success" href="#rsi" role="button" data-toggle="collapse" data-target="#demo">View RSI</button>
<div id="rsi" class="collapse">
```{r 'Add RSI'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add RSI") 
```

<strong>Oversold in Uptrend</strong>
[type = stock] AND [country = US] 
AND [Daily SMA(20,Daily Volume) > 40000] 
AND [Daily SMA(60,Daily Close) > 20] 

AND [Daily Close > Daily SMA(200,Daily Close)] 
AND [Daily RSI(5,Daily Close) <= 30]
<strong>RSI Overbought in Downtrend</strong>
[type = stock] AND [country = US] 
AND [Daily SMA(20,Daily Volume) > 40000] 
AND [Daily SMA(60,Daily Close) > 20] 

AND [Daily Close < Daily SMA(200,Daily Close)] 
AND [Daily RSI(5,Daily Close) >= 70]
```{r 'Add RSI Indicator',eval=F}
# ----------------------- Tue Apr 24 11:26:17 2018 ------------------------#
# Needs refinemnt of factor indicators
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add RSI Indicator") 
```
</div>


<button class="btn btn-sm btn-success" href="#roc" role="button" data-toggle="collapse" data-target="#demo">Add Rate of Change and Momentum</button>
<div id="roc" class="collapse">
### Add Rate of Change (ROC) and Momentum
<ul>
  <li><a href="http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:rate_of_change_roc_and_momentum" target="_blank">ChartSchools: ROC</a></li>
  <li><a href="https://www.investopedia.com/articles/technical/092401.asp" target="_blank">Investopedia: ROC 101</a></li>
</ul>


```{r 'Add ROC and Momentum'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add ROC And Momentum") 
```
</div>
### Add Average True Range
<button class="btn btn-sm btn-success" href="#atr" role="button" data-toggle="collapse" data-target="#demo">View ATR</button>
<div id="atr" class="collapse">
<ul>
  <li><a href="http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_true_range_atr" target="_blank">StockCharts: ATR</a></li>
  <li><a href="https://www.investopedia.com/articles/trading/08/average-true-range.asp" target="_blank">Investopedia: Using ATR</a></li>
</ul>
ATR is best used to set a stop loss at the daily low - 1ATR
```{r 'Add ATR'}
# ----------------------- Sat May 05 10:15:47 2018 ------------------------#
# Average True Range could be useful for setting stop-loss
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add ATR")
```
</div>

### Add SAR
<button class="btn btn-sm btn-success" href="#sar" role="button" data-toggle="collapse" data-target="#demo">View SAR</button>
<div id="sar" class="collapse">

```{r 'Add SAR'}
# ----------------------- Sat May 05 10:15:47 2018 ------------------------#
# Average True Range could be useful for setting stop-loss
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add SAR")
```
<strong>Break Above Falling SAR</strong>
AND [Yesterday's Daily High < Yesterday's Daily Parabolic SAR(0.01,0.2)] 
AND [Daily High > Daily Parabolic SAR(0.01,0.2)]
<strong>Break Below Rising SAR</strong>
AND [Yesterday's Daily Low > Yesterday's Daily Parabolic SAR(0.01,0.2)] 
AND [Daily Low < Daily Parabolic SAR(0.01,0.2)]
```{r 'Add SAR Indicator'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add SAR Indicator")
```
```{r 'SAR Tune'}
sarParam_df <- expand.grid(af = c(seq(.01,.2,.01)), mf = c(5,10,20,50)) %>% dplyr::mutate(maf = af*mf) %>% dplyr::select(-mf)

```
</div>
<button type="button" data-toggle="collapse" data-target="#macd" class="btn btn-outline-primary">View MACD</button>
<div class="collapse" id="macd">


### Add MACD Compressed

```{r 'Add MACD Compressed'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add MACD Compressed")
```
</div>
<button type="button" data-toggle="collapse" data-target="ephemeris" class="btn btn-outline-primary">View Ephemeris</button>
<div class="collapse" id="ephemeris">

### Add Ephemeris
```{r 'Add Ephemeris'}
knitr::read_chunk("JobsScripts/AddAllVariablestoData.R", labels = "Add Ephemeris Data")

```
</div>


# Response Variables
## Test Trailing Stop Loss
```{r 'Test Trailing Stop Loss Fn'}
source("JobsScripts/QuantFunctions_TSL.R")
rstudioapi::jobRunScript("JobsScripts/TestTSL.R",workingDir = getwd(), exportEnv = "R_GlobalEnv")
```
```{r 'Add TSL as RV'}
rstudioapi::jobRunScript("JobsScripts/AddTSLRV.R",workingDir = getwd(), exportEnv = "R_GlobalEnv")
```




```{r 'Fns for working with Position_ts', eval=F}
#Load Position_ts from csv Backups
pdatapath <- "~/R/Quant/PositionData"
datetime <- list.files(path = pdatapath)[stringr::str_which(list.files(path = pdatapath), "_rv\\.csv$")] %>% stringr::str_extract("\\d{4}-\\d{2}-\\d{2}(?=\\_rv\\.csv)") %>% lubridate::ymd() %>% max() %>% as.character() 
Positions_ts <- list.files(path = pdatapath)[stringr::str_which(list.files(path = pdatapath), paste0(datetime,"_rv\\.csv$"))] %>% stats::setNames(object = ., stringr::str_extract(., "^[A-Z]+")) %>%  purrr::imap(function(.x, .y){
  out <- readr::read_csv(file = paste0("PositionData/",.x)) %>% dplyr::mutate_at( dplyr::vars(time, dplyr::ends_with("_ind")), dplyr::funs(as.POSIXct, .args = list(origin = lubridate::origin)))
  attr(out, "Sym") <- .y
  out
})
# Change columns to appropriate types and add the attributes
Positions_ts %<>% purrr::imap(function(.x, .y){ # Actual Time for measuring exact weeks, quarters and approx. months
  td_nm <- stringr::str_extract(names(.x), "^time$|^date$") %>% .[1]
  if (is.numeric(.x[, td_nm])){
    .x <- dplyr::mutate_at(.x, dplyr::vars(time), dplyr::funs(as.POSIXct, .args = list(origin = lubridate::origin)))}
  if (xts::is.xts(.x)){
    out <- .x
#out <- xts(order.by = seq(min(time(.x)), max(time(.x)), by = "1 days")) %>% merge.xts(l) %>% na.locf()
xts::xtsAttributes(out) <- list(Sym = .y)
  } else {
    out <- .x
    #out <- data.frame(time = seq(min(out[, td_nm, drop = T]), max(out[, td_nm, drop = T]), by = "1 days"), stringsAsFactors = F) %>% left_join(.x) %>% na.locf 
  out <- tibbletime::tbl_time(out, index = !!td_nm)
  attr(out, "Sym") <- .y
}
return(out)
})

#Remove extraneous macd that snuck in and has been effing up models
Positions_ts <- purrr::map(Positions_ts, function(.x){
  .x[,-stringr::str_which(colnames(.x), "^macd$")]
})
#Remove chg and change
Positions_ts <- purrr::map(Positions_ts, function(.x){
  .x[,-stringr::str_which(colnames(.x), "^chg$|^change$")]
})

# Fix irregular column names from AddRVstoData function, may be used again if not fixed
Positions_ts <- purrr::map(Positions_ts, function(.x){
  Time.ind <- which(purrr::map(.x,class) %>% unlist == "Date")[2]
  cl.nm <- seq(Time.ind, ncol(.x) - 1)
  nms <- names(.x)[cl.nm]
  out <- .x[, - Time.ind]
  names(out)[cl.nm] <- nms
  return(out)
} )
# Save Positions Individually
purrr::map2(.x = Positions_ts, .y = names(Positions_ts), function(.x, .y){
  print(.y)
  if (!xts::is.xts(.x)) {
  readr::write_csv(.x, path = paste0("PositionData/",.y,min(.x$Time),"_",max(.x$Time),".csv"))
    } else {
    zoo::write.zoo(.x, file = paste0("PositionData/",.y,stats::time(.x) %>% min,"_",stats::time(.x) %>% max,".csv"), index.name = "Time", col.names = T)
  }
})
```


```{r 'Restoring Discrepancies in recent_prices'}
bymin <- list.files(path = "PositionData", pattern = "bymin", full.names = T) %>% purrr::imap(fT = function(time, Time) {
  message(length(time))
  message(sum(is.na(Time)))
  message(sum(is.na(time)))
 out <- append(as.character(time[!is.na(time)]), as.character(Time[!is.na(Time)]))
  lubridate::ymd_hms(out)
}, .f = function(.x, .y, fT) {
  out <- utils::read.csv(.x) %>% dplyr::mutate(time = fT(time,Time)) %>% dplyr::select(- Time)
  readr::write_csv(out, path = .x, col_names = T)
  })
```


# Generic Fn to add all Signals and Response Variables



```{r 'Add Signals to Extended dataset and trim to match Position_ts'}
#params$dataUtil()
dat <- Positions_ext
# Set the signal windows desired
wind  <-  params$wind
# Save the file
save(dat, wind, file = "dat.Rdata")
rstudioapi::jobRunScript("JobsScripts/AddIVstoData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
Positions_ts <- purrr::map2(Positions_ts, dat, function(.x, .y){
  s <- attr(.x, 'Sym')
  if (xts::is.xts(.x) & xts::is.xts(.y)) {
    out <- stats::window(.y, start = min(stats::time(.x)))
  } else {
     td_nm <- stringr::str_extract(names(.x), "^time$|^date$") %>% subset(subset = !is.na(.)) %>% .[1]
    out <- .y[.y[[td_nm]] >= min(.x[[td_nm]]), ]
  }
  attr(out, 'Sym') <- s
  return(out)
})
# Once Completed return the variable to it's original name and save it
assign("Positions_ts_iv", Positions_ts , envir = globalenv()) 
```

```{r 'Add RVs to Data'}
dat <- Positions_ts
# Set the signal windows desired in parameters.r
source("~/R/Quant/JobsScripts/parameters.R")
add <- F
save(dat, TSLvars, add, file = "~/R/Quant/dat.Rdata")
rstudioapi::jobRunScript("JobsScripts/AddRVstoData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
# Save
```

```{r 'Load Positions_ts_rv from HD'}
Positions_ts_rv <- purrr::map(Positions_v, function(.x){
  message(paste0("Finding files: ", .x))
    .a_f <- list.files("~/R/Quant/PositionData", pattern = .x, full.names = T)
    if (.x == "GOOG") .rvs <- .a_f[.a_f %>% stringr::str_which("GOOG(?!L).*rv\\.csv$")] else .rvs <- .a_f[.a_f %>% stringr::str_which("rv\\.csv$")]
      message(paste0("Reading data: ", .x))
      .rv_data <- readr::read_csv(.rvs)
  if (dplyr::select(.rv_data, dplyr::ends_with("ind")) %>% purrr::map(~ is.numeric(.x)) %>% any) {
    .rv_data <- dplyr::mutate_at(.rv_data, dplyr::vars(dplyr::ends_with("ind")), ~ lubridate::as_datetime(., tz = "EST"))
  }
      td_nm <- stringr::str_extract(names(.rv_data), stringr::regex("^time$|^date$", ignore_case = T)) %>% purrr::keep(~ !is.na(.x)) %>% .[1]
      #If any of the sell indexes are greater than the maximum time index then make them the max time index. Necessary to make up for the deletion of a row due to the artifact.
      t_max <- max(.rv_data[[td_nm]])
  if (any(dplyr::select(.rv_data, dplyr::ends_with("ind")) %>% purrr::map(~ max(.x, na.rm = T)) > t_max)) {
    message("Some rv index values exceed the time scale - fixing")
    .rv_data <- .rv_data %>% dplyr::rowwise() %>% dplyr::mutate_at( dplyr::vars(dplyr::ends_with("ind")), ~ {
      
      if (is.na(.x) | .x > t_max) .x <- NA else .x
      .x
    })
  }
    attr(.rv_data, "Sym") <- .x
   # Remove wierd artifact column
    # .rv_data <- .rv_data[- nrow(.rv_data), ]

    # readr::write_csv(.rv_data, path = paste0("~/R/Quant/PositionData/",.x , min(.rv_data[[td_nm]]) %>% lubridate::as_date(), "_", max(.rv_data[[td_nm]]) %>% lubridate::as_date(), "_rv.csv"))
     return(.rv_data)
})

purrr::map(Positions_ts_rv, function(.x){
  ind <- .x %>% dplyr::select(dplyr::ends_with("ind"))
  purrr::map_dfr(ind, ~ range(.x))
})
```

```{r 'Add Gain Mix Period RVs'}
# Deprecated
save(dat, file = "dat.Rdata")
rstudioapi::jobRunScript(path = "JobsScripts/GainMixPeriod.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv")
# Save
Positions_ts <- dat
rstudioapi::setCursorPosition(SL)

```

```{r 'Map Returns'}
pct <- seq(.00, .2, .01)
names(pct) <- seq(.00, .2, .01)
dat <- Positions_ts_rv
.opts <- list(bs.v = T, with.gains = F, max.gain = T)
save(dat, pct, .opts, file = "dat.Rdata")
rstudioapi::jobRunScript(path = "JobsScripts/MapReturns.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv")


```

```{r 'Join RV & IV'}
Positions_ts_iv <- purrr::imap(Positions_ts_iv, function(.x,.y){attr(.x, "Sym") <- .y
return(.x)})

Positions_ts_rv_iv <- purrr::map2(Positions_ts_iv[names(Positions_ts_rv)], Positions_ts_rv, function(.x, .y){
  s <- attr(.x, 'Sym')
  if (xts::is.xts(.x) & xts::is.xts(.y)) {
    out <- xts::merge.xts(.x, .y, join = "left")
  } else {
     nms <- stringr::str_extract(names(.x), "^time$|^date$|^open$|^high$|^low$|^close$|^volume$") %>% subset(subset = !is.na(.)) 
    out <- dplyr::left_join(.x, .y, by = nms)
    out <- tibbletime::tbl_time(out, index = !! params$getTimeIndex(out)) 
  }
  attr(out, 'Sym') <- s
  return(out)
})
```



```{r 'View Optimized Returns'}
Positions_ret <- ret
save(Positions_ret, file = "Positions_ret.Rdata", compress = "xz")
load("Positions_ret.Rdata")
# ----------------------- Thu Jul 18 17:46:44 2019 ------------------------#
#Best returns

best.returns <- Positions_ret %>% purrr::map_depth(.depth = 2, .f = function(.x){
  out <- list()
  out$trades <- purrr::map(.x, .f = magrittr::extract, "trades") %>% purrr::map(.f = `[[`, 1)
  out$trades <- purrr::map(out$trades, function(.x){
    nr <- nrow(.x)
    if(HDA::go("nr")) {
    out <- cbind(.x,start = seq(1, nr))
    } else out <- .x
  return(out)})
  out$trades <- data.table::rbindlist(out$trades, idcol = T)
  
  out$returns <- purrr::map(.x, .f = magrittr::extract, "returns") %>% purrr::map(.f = `[[`, 1) %>% purrr::map(as.data.frame)
  out$returns <- data.table::rbindlist(out$returns, idcol = T)
  out <- cbind.data.frame(out$trades, out$returns %>% dplyr::select(- ".id"))
  return(out)
  }) %>% purrr::map(~data.table::rbindlist(., idcol = T)) %>% data.table::rbindlist(., idcol = T)
names(best.returns)[c(1:3)] <- c("sym","tsl","pct")
best.returns %>% dplyr::group_by(sym, tsl) %>% dplyr::filter(Cum.Returns == max(Cum.Returns))
#End best returns
 #----------------------- Thu Jul 18 17:47:20 2019 ------------------------#
# ----------------------- Thu Jul 18 18:02:42 2019 ------------------------#
#Positions_tsl
Positions_tsl <- Positions_ret %>% purrr::map_depth(.depth = 2, .f = function(.x){
  out <- list()
  out$trades <- purrr::map(.x, .f = magrittr::extract, "trades") %>% purrr::map(.f = `[[`, 1)
  out$trades <- purrr::map(out$trades, function(.x){
    nr <- nrow(.x)
    if(HDA::go("nr")) {
    out <- cbind(.x,start = seq(1, nr))
    } else out <- .x
  return(out)})
  out$trades <- data.table::rbindlist(out$trades, idcol = T)
  
  out$returns <- purrr::map(.x, .f = magrittr::extract, "returns") %>% purrr::map(.f = `[[`, 1) %>% purrr::map(as.data.frame)
  out$returns <- data.table::rbindlist(out$returns, idcol = T)
  out <- cbind.data.frame(out$trades, out$returns %>% dplyr::select(- ".id"))
  return(out)
  }) %>% purrr::map(~data.table::rbindlist(., idcol = T))
Positions_tsl <- Positions_tsl %>% purrr::map(function(.x){
  names(.x)[c(1,2)] <- c("tsl","pct")
  .x %<>% dplyr::mutate_at(dplyr::vars(pct),~as.numeric(.)) 
return(.x)
})
Positions_tsl_maximums <- purrr::map(Positions_tsl, function(.x){
  out <- dplyr::inner_join(.x %>% dplyr::group_by(tsl,pct) %>% dplyr::filter(Cum.Returns == max(Cum.Returns)) %>% dplyr::summarise_at(dplyr::vars(dplyr::contains("."),dplyr::contains("%"), Returns),~mean), .x %>% dplyr::group_by(tsl) %>% dplyr::summarize(`75%max` = mean(`75%max`),`100%max` = mean(`100%max`)) %>% dplyr::arrange(dplyr::desc(`75%max`)), by = c("tsl")) %>% dplyr::select(- dplyr::ends_with(".x")) %>% dplyr::rename_at(dplyr::vars(dplyr::ends_with(".y")), ~ list(stringr::str_replace(., pattern = "\\.y", replacement = ""))) %>% dplyr::arrange(dplyr::desc(Cum.Returns))
  return(out)
})
save(Positions_tsl, file = paste0("Positions_tsl",lubridate::today(),".Rdata"))
#End Positions_tsl
 #----------------------- Thu Jul 18 18:02:54 2019 ------------------------#
```
```{r 'May_June 2019 TSL Performance'}

tsl_performance <- purrr::map_depth(ret, .depth = 3, .f = function(.x) {
  out <- c(Returns = attr(.x, "Returns"),Cum.Returns = attr(.x, "Cum.Returns"))
  return(out)
}) %>% purrr::map_depth(.depth = 2, .f = function(.x){
  .x <- subset(.x, subset = purrr::map(.x,length) %>% unlist != 0)
  if(length(.x) == 0) return(NULL)
  d <- do.call("rbind",.x)
  if (stringr::str_detect(colnames(d),"Returns") %>% any){
    #TODO Add the top quartile rather than just the single max
  ind <- which.max(d[,"Returns", drop = T])
  pct <- rownames(d)[ind]
  returns <- d[ind,"Returns", drop = T]
  } else returns <- 0
  if (stringr::str_detect(colnames(d),"Cum.Returns") %>% any){
  ind <- which.max(d[,"Cum.Returns", drop = T])
  pct <- rownames(d)[ind]
  cum.returns <- d[ind,"Cum.Returns", drop = T]} else cum.returns <- 0
  out <- c(pct = pct, Returns = returns, Cum.Returns = cum.returns)
}) %>% purrr::map(function(.x){
  d <- do.call("rbind",.x) 
  rd <- rownames(d) 
  d %<>% as.data.frame() 
  d %<>%  dplyr::mutate_all(dplyr::funs(as.numeric(as.character(.))))
  row.names(d) <- rd
  d %<>% tibble::rownames_to_column()
  if (identical(d %>% dplyr::arrange(dplyr::desc(Returns)) %>% .[1,2],d %>% dplyr::arrange(dplyr::desc(Cum.Returns)) %>% .[1,2])){
  out <- d %>% dplyr::arrange(dplyr::desc(Returns)) %>% .[1,]
  } else {
      out <-  rbind.data.frame(d %>% dplyr::arrange(dplyr::desc(Returns)) %>% .[1,],d %>% dplyr::arrange(dplyr::desc(Cum.Returns)) %>% .[1,])
  }
  return(out)
 })
```
 
```{r 'Add Benchmarks'}
load(file = params$paths$Positions_tsl)
best_tsl <- purrr::map2(Positions_tsl[names(dat)], .y = dat, tslv = params$TSLvars, function(.x, .y, tslv){
  .x %<>% dplyr::mutate_at(dplyr::vars(tsl),~ stringr::str_replace(.,"\\_rv$",""))
  .x %<>% dplyr::group_by(tsl, pct) %>% dplyr::summarise(Cum.Returns = mean(Cum.Returns), `75%max` = mean(`75%max`),`100%max` = mean(`100%max`)) %>% dplyr::mutate_at(dplyr::vars(dplyr::contains("%")), ~ as.numeric(.))
  # Get the TSL with the most cumulative returns and with the most possible limit gain for a given period
  out <- list(tsl_types = rbind.data.frame(dplyr::arrange(.x, dplyr::desc(Cum.Returns)) %>% .[1, c("tsl", "pct", "75%max", "100%max", "Cum.Returns")],
  dplyr::arrange(.x, dplyr::desc(`100%max`)) %>% .[1, c("tsl", "pct", "75%max", "100%max", "Cum.Returns")]) %>% dplyr::ungroup())
  # Are there tsl not already represented in the data?
  existing_tsl <- {out$tsl_types$tsl[out$tsl_types$tsl %in% {names(.y)[stringr::str_which(names(.y),"rv$")] %>% stringr::str_replace(.,"\\_rv$","")}] %>% length} > 0
  # If there are and add is true (defaults to true) then add only those that don't already exist
  if (existing_tsl){
  # filter the table for those tsl not already in the data
  out$tsl_types %<>% dplyr::filter(!tsl == existing_tsl) }
  # Return the TSLvars parameters for each of those TSL types
  out$tslv <- tslv[out$tsl_types[["tsl"]]]
  out$tsl_types %<>% dplyr::mutate_at(dplyr::vars(pct), ~ as.numeric(.)) 
  return(out)
  })
dat <- Positions_ts_rv_iv
save(dat, best_tsl, file = "dat.Rdata")
rstudioapi::jobRunScript(path = "JobsScripts/AddBenchmarks.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv")
best_tsl <- ret
save(best_tsl, file = "best_tsl.Rdata")
```
```{r 'Examine overall performance of various ML Methods'}
rstudioapi::jobRunScript(path = "MdlBkp/CaretTuningGrids.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv") 
```


```{r 'Tuning Grids and MethodList'}
methodlist <- c("xgbTree", "xgbDART", "mlpWeightDecayML", "bagEarth")
tGs <- purrr::map(overall.performance, methodlist = methodlist, function(.x, methodlist){
  .x <- .x[names(.x) %in% methodlist]
  out <- purrr::imap(.x, function(.x, .y){
    pns <- as.character(caret::modelLookup(stringr::str_extract(.y, "\\w+"))$parameter)
    names(pns) <- as.character(caret::modelLookup(stringr::str_extract(.y, "\\w+"))$parameter)
    tG <- purrr::map(pns, op = .x, function(.x, op){
  unique(op[[.x]]) %>% na.omit %>% unclass
}) %>% expand.grid
    return(tG)
  })
  return(out)
})


modelSpecs <- purrr::map(tGs, ml = methodlist, function(.x, ml){
  out <- purrr::imap(.x, ml = ml, function(.x, .y, ml){
    if (any(.y %in% ml)){
    out <- caretEnsemble::caretModelSpec(method = .y, tuneGrid = .x )      
    } else {
      out <- NULL
    }
    return(out)
  }) %>% purrr::compact()
  return(out)
})
tuningGrids <- list(
mlpWeightDecayML = list(layer1 = c(1,3,5,7,9), layer2 = c(0,1,3,5,7,9), layer3 = c(0,1,3,5,7,9), decay = .1/10^seq(1,4)) %>% expand.grid
)
#mlpSGD.tG <- overall.performance[["FB"]][["mlpSGD"]]
modelSpecs <- purrr::map(modelSpecs, tG = tuningGrids, mlist = methodlist, function(.x, tG, mlist){
  mS <- .x
  pf <- sys.frame(sys.nframe())
  # If there are models in methodlist missing from the current specified modelspecs
  if (any(!mlist %in% names(mS))) {
  mS <- append(mS, purrr::map(mlist[!mlist %in% names(mS)], ~ caretEnsemble::caretModelSpec(method = ., tuneLength = 7))) %>% stats::setNames(nm = c(names(mS),mlist[!mlist %in% names(mS)]))
  } 
  if (any(purrr::imap_lgl(mS[names(tG)], pf = pf, function(.x, .y, pf){
     is.null(.x$tuningGrid) & !is.null(pf$tG[[.y]])
  }))) { #if there are unset tuning grids in the modelspec for which tuning grids have been specified manually
    mS[names(tG)] <- purrr::imap(mS[names(tG)], pf = pf, function(.x,.y, pf){
      .x$tuneLength <- NULL
      .x$tuneGrid <- pf$tG[[.y]]
      return(.x)
    })
  }
  return(mS)
})
# Get names not represented in modelSpecs thus far
nms <- dplyr::setdiff(names(Positions_ts_rv),names(modelSpecs)) %>% stats::setNames(dplyr::setdiff(names(Positions_ts_rv),names(modelSpecs)))
# Append new caretModelSpecs
modelSpecs <- append(modelSpecs, purrr::map(nms, mS = modelSpecs[[1]], function(.x, mS){
  out <- purrr::imap(mS, function(.x, .y){
    if (is.null(.x$tuneGrid)) {
      .x$tuneLength <- 7
      }
    .x$method <- stringr::str_extract(.y, "\\w+")
    
    return(.x)
  })
  return(out)
}))
# add PCA to multilayer perceptron
modelSpecs <- purrr::map(modelSpecs, function(.x){
  out <- purrr::imap(.x, function(.x,.y){
    if (stringr::str_detect(.y ,"mlp"))  .x$preProcess <- "pca"
    return(.x)
  })
})
# Sort
modelSpecs <- modelSpecs[names(Positions_ts_rv)]
```

```{r 'Machine Learning Algorithm Training'}
# Recommended to clear environment and then load data so as to free up all RAM
rm(list = ls())
load(file = params$paths$Positions_ts_rv_iv)
load(file = params$paths$Positions_tsl)
#methodList <- c("xgbTree", "C5.0", "mlpKerasDropout", "mlpKerasDecay") # Add method names. See http://topepo.github.io/caret/available-models.html for details
    # If first run, use tunelength. If final or subsequent runs with a refined tuning grid, provide tuneGrids in the lapply call (not in it's function)

dat <- Positions_ts_rv_iv[1] # Pass the list of timeseries objects for which algorithms will trained
train_rvs <- NULL
best_tsl <- best_tsl[names(dat)[1]]
modelSpecs <- modelSpecs[names(dat)[1]]
# .replace - boolean indicating whether to replace existing models or to simply add new ones (FALSE)
.replace <- F
save(modelSpecs, dat, best_tsl, .replace, file = "dat_dt.Rdata")
# ----------------------- Mon Jun 03 06:12:02 2019 ------------------------#
# Important to filter all none profitable signals from the type prediction
rstudioapi::jobRunScript(path = "JobsScripts/TrainModels.R", workingDir = getwd(), importEnv = F,exportEnv = "R_GlobalEnv")

```
```{r 'Remove gain and mix type or other objects'}

total.time <- purrr::map2(c('TSLA','AMD','BYND','TSM','MSFT','XSW','CGC','TLRY','UNH','NFLX','GRPN','V','SQ','PYPL'), best_tsl[c('TSLA','AMD','BYND','TSM','MSFT','XSW','CGC','TLRY','UNH','NFLX','GRPN','V','SQ','PYPL')], function(.x, .y){
  .fn <- list.files(path = "~/R/Quant/MdlBkp", pattern = paste0(.x,"_cl.Rdata"), full.names = T)
  # Get the name of the object
  ob_chr <- paste0(.x, "_cl")
  message(paste0("Loading: ", .x))
  # Load the data
  .st <- system.time({
     try({load(.fn)})
  })
  message(paste0(.x, ": Load time - ", lubridate::as.duration(.st[3])))
  ob <- get0(ob_chr)
  message(paste0("Names before: ", paste0(names(ob), collapse = ", ")))
  .ob_nms <- names(ob)
  .tsl <- unique(paste0(.y$tsl_types$tsl,"_rv"))
  if (!all(.tsl %in% names(ob))) .out <- .tsl[!.tsl %in% names(ob)] else .out <- NULL
  if (any(!names(ob) %in% .tsl)){
  .keep <- purrr::map_lgl(ob, function(.x){
    stringr::str_detect(.x[[1]][["coefnames"]], "LI$") %>% any
    })
  message(paste0("keeping: ", paste0(names(.keep)[.keep], collapse = ", ")))
  ob <- ob[.keep]
  }
  ob <- ob[!names(ob) %in% c("Mix_type", "Gain_type")]
  message(paste0("Names after: ", paste0(names(ob), collapse = ", ")))
  if (all(.ob_nms %in% names(ob))) return(.out)
  assign(ob_chr, ob)
  message(paste0(.x, ": Saving..."))
    # .proceed <- readline("Proceed?")
    # if (.proceed == "n") stop("Do not proceed.") 
  .st <- system.time({
     save(list = ob_chr, file = .fn, compress = "bzip2")
  })
 return(.out)
})
```


```{r 'Examine Ensemble Models'}
res <- purrr::map2(PYPL_cl[[3]], PYPL_cl[[4]], ~ .x$results %>% dplyr::mutate(id = 1) %>% rbind.data.frame(.y$results %>% dplyr::mutate(id =2)))
purrr::map(res, ~ table(utils::head(.x %>% dplyr::arrange(RMSE))$id))
model_nms <- list.files("MdlBkp/", full.names = T, include.dirs = T)
load(model_nms[6])
```



```{r 'Examine Models'}
save(dat_dt, file = paste0("xgBoost",Positions_ts$GOOG %>% time %>% min,"_",Positions_ts$GOOG %>% time %>% max,".Rdata"))
Model_Perf <- parallel::mclapply(Positions_dt, function(dt){
  if(is.null(dt)) return(NULL)
  names(dt) <-  purrr::map(dt, purrr::pluck, "terms", 2) %>% as.character()
  mdl_performance <- data.table::rbindlist(purrr::map2(.x = dt, .y = names(dt), function(.x, .y){
    out <- purrr::pluck(.x,"results") %>% dplyr::mutate(Model = .y)
    return(out)})
  )
  out <- list()
  out$mdl_imp <- purrr::map(.x = dt, function(.x){
    mod <- purrr::pluck(.x,"finalModel")
    out <- list()
    out$mod_imp <- xgboost::xgb.importance(mod[["feature_names"]],mod)
    out$mod_plot <- xgboost::xgb.ggplot.importance(out$mod_imp)
    return(out)
    })
  out$mdl_performance <- mdl_performance %>% dplyr::mutate(Mod_Rank = dplyr::percent_rank(Rsquared)) %>% dplyr::arrange(dplyr::desc(Mod_Rank))
  return(out)
  })
# Remove training data to create a smaller file
Positions_dt %<>% purrr::map(function(l){
  l[[1]][["trainingData"]] %<>% .[, stringr::str_which(names(.),".outcome|Index")]
  return(l)
})
# Find Correlated Stocks for Backtesting algorithms:
# http://www.sectorspdr.com/sectorspdr/tools/correlation-tracker
```


```{r 'Get GRPN Historical'}
googlesheets::gs_auth(token = "~//R//sholsen_googlesheets_token.rds")
gsPositions <- googlesheets::gs_url("https://docs.google.com/spreadsheets/d/1Iazn6lYRMhe-jdJ3P_VhLjG9M9vNWqV-riBmpvBBseg/edit#gid=0")
GRPN <- googlesheets::gs_read(gsPositions, ws = "GRPN")
GRPN[, c('open','high','low','close','volume')] <- quantmod::OHLCV(GRPN) %>% apply(1, function(r){
  r[stringr::str_detect(r,"REF")] <- NA
  return(r)
}) %>% t %>% zoo::na.locf()
GRPN %<>% dplyr::mutate_at(dplyr::vars(c(2:6)),dplyr::funs(as.numeric))
GRPN$changePercent <- purrr::map_dbl(1:nrow(GRPN), GRPN = GRPN, function(.x, GRPN){
  if (stringr::str_detect(GRPN[.x, "changePercent"], "REF")) {
  out <- (GRPN[.x,c("close"), drop = T] - GRPN[.x - 1,c("close"), drop = T]) / GRPN[.x - 1,c("close"), drop = T] 
  } else {out  <- GRPN[.x, c("changePercent"), drop = T] %>% as.numeric}
  return(out)
})
attr(GRPN, "Sym") <- "GRPN"
Positions_ts$GRPN <- tibbletime::as_tbl_time(GRPN, index = "Time") 
```
```{r 'Update BYND'}
BYND <- googlesheets::gs_read(gsPositions, ws = "BYND")
BYND[, c('open','high','low','close','volume')] <- quantmod::OHLCV(BYND) %>% apply(1, function(r){
  r[stringr::str_detect(r,"REF")] <- NA
  return(r)
}) %>% t %>% zoo::na.locf()
BYND %<>% dplyr::mutate_at(dplyr::vars(c(2:6)),dplyr::funs(as.numeric))
BYND$changePercent <- purrr::map_dbl(1:nrow(BYND), BYND = BYND, function(.x, BYND){
  if (stringr::str_detect(BYND[.x, "changePercent"], "REF")) {
  out <- (BYND[.x,c("close"), drop = T] - BYND[.x - 1,c("close"), drop = T]) / BYND[.x - 1,c("close"), drop = T] 
  } else {out  <- BYND[.x, c("changePercent"), drop = T] %>% as.numeric}
  return(out)
})
BYND <- tibbletime::as_tbl_time(BYND, index = "Time") 
attr(BYND, "Sym") <- "BYND"
Positions_ts$BYND <- BYND
```

# Test Out of Time
## Create OOT & Add Signals 
```{r 'Gather new Data from Portfolio Sheet', eval = F}
# Deprecated - Google Finance is unreliable
gsPositions <- googlesheets::gs_url("https://docs.google.com/spreadsheets/d/1Iazn6lYRMhe-jdJ3P_VhLjG9M9vNWqV-riBmpvBBseg/edit#gid=0")
shts <- purrr::map(.x = c('Holdings', '(Hi)' , '(Lo)', '(Op)', '(Vo)', '(cP)'), gs = gsPositions, .f =  function(.x, gs){
    out <- googlesheets::gs_read(gs, ws = .x, trim_ws = T)
    Sys.sleep(1)
    return(out)
    })
shts <- purrr::map(shts,function(.x){
    .x$Date %<>% lubridate::mdy_hms() %>% format("%m/%d/%Y") %>% lubridate::mdy()
    return(.x)
  })
Positions_v <- names(Positions_ts)
names(Positions_v) <- names(Positions_ts)
Positions_new <- purrr::map(Positions_v, shts = shts, function(.x, shts){
  
  
    clm <- purrr::map(shts, purrr::pluck, .x)
    clm <- do.call('cbind', clm) %>% as.data.frame
    
    names(clm) <- c("close","high","low","open","volume","changePercent")
    
    toNum <- function(x){
  x %>% stringr::str_replace_all("\\$|\\,","") %>% as.numeric
    }  
    clm %<>% dplyr::mutate_if(.pred = dplyr::funs(is.factor),dplyr::funs(toNum))
    dt.clm <- purrr::pluck(shts[[1]],"Date")
    out <- cbind.data.frame(Time = dt.clm, clm) 
    out %<>% na.omit
    #Fill dates for proper windowing
   out <- dplyr::left_join(data.frame(Time = seq(out$Time %>% min,out$Time %>% max, "1 days")), out) %>% na.locf %>% tibbletime::tbl_time(index = "Time")
  out$Dec_date <- tibbletime::get_index_col(out) %>% lubridate::decimal_date() %>% {. - as.numeric(stringr::str_match(., "^\\d+"))}
    attr(out,"Sym") <- .x
  return(out)
})
```



```{r 'Add Signals to New Data'}
dat <- params$getPositions_new(Positions_v, params)
# Set the signal windows desired
# wind  <-  c(weeks = 7, moonphase = 7*2, mooncycle = 7*4, quarters = 7*4*3)
# Save the file
save(dat, file = "dat.Rdata")
rstudioapi::jobRunScript("JobsScripts/AddIVstoData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
```
```{r 'Add RVs to New Data'}
# After the previous completes
wind
save(dat, wind, TSLvars, best_tsl, file = "dat.Rdata") #TSLvars from Chunk 35: Add RVs to Data
rstudioapi::jobRunScript("JobsScripts/AddRVstoNewData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
Positions_new <- dat
save("Positions_new", file = paste0("Positions_new.Rdata"))
```

```{r 'Test ML Ensemble Models with OOT Data'}
# Supply if OOT data is later than training data, if OOT data is older than training data do not supply

OOTbegin <- Positions_ts_rv_iv[[1]][[params$getTimeIndex(Positions_ts_rv_iv[[1]])]] %>% max
dat <- params$getPositions_new(Positions_v, params)
source("~/R/Quant/JobsScripts/AddIVstoData.R", local = T)
# Add Dependent variables
source("~/R/Quant/JobsScripts/AddRVstoNewData.R", local = T)
fns <- list.files("~/R/Quant/MdlBkp", full.names = T, pattern = ".Rdata$")
names(fns) <- stringr::str_match(fns, "\\/MdlBkp\\/([A-Z]+)")[,2]
fns <- fns[names(dat)]
#best_tsl <- Positions_tsl
best_tsl <- best_tsl[names(dat)]
save(dat, best_tsl, fns, OOTbegin, file = "dat.Rdata")
rstudioapi::jobRunScript("JobsScripts/ApplyModelstoNewData.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)
purrr::map(dat, attr, "Cum.Returns")
Positions_strat <- dat
```

```{r 'Automated OOT Data Test'}
# ----------------------- Tue Jun 18 10:02:30 2019 ------------------------#
# Run the OOT data script
rstudioapi::jobRunScript("JobsScripts/AutomatedOOTDataTest.R", workingDir = getwd(), exportEnv = "R_GlobalEnv", importEnv = F)

```

```{r 'Create Automated OOT Test Task'}

rscript <- "C:/Users/Administrator/Documents/R/Quant/JobsScripts/AutomatedOOTDataTest.R"
taskscheduleR::taskscheduler_create(basename(rscript),rscript, schedule = "ONCE", starttime = "06:16", rscript_args = "--verbose",debug = T)
tasks <- taskscheduleR::taskcheduler_runnow("AutomatedOOTDataTest.R")

times <- 09:15 %>% purrr::map(function(.x){
  if(nchar(.x)<2) return(stringr::str_replace(.x,"^","0")) else {
    return(.x)
    }}) %>% unlist %>% paste0(":55")
days  <- c("MON", "TUE",
  "WED", "THU", "FRI")
purrr::map(times, days = days, rscript = rscript, function(.x, days, rscript){
  out <- purrr::map(days, time = .x, rscript = rscript, function(.x, time, rscript){
    taskname = paste0(basename(rscript), "_",.x,"_",stringr::str_replace(time,"\\:",""))
    print(taskname)
    desc <- taskscheduleR::taskscheduler_create(taskname, rscript,
  schedule = "WEEKLY", starttime = time,
  startdate = format(Sys.Date(), "%m/%d/%Y"), rscript_args = "--verbose", days = .x, debug = T)
    return(c(Task = taskname, Desc = desc))
  })
  return(out)
}) %>% purrr::map(function(.x)do.call(rbind,.x)) %>% do.call(rbind,.) %>% assign(x = paste0(basename(rscript)), ., envir = globalenv())
save(list = paste0(basename(rscript)), file = paste0(basename(rscript),".Rdata"))
tasks <- taskscheduleR::taskscheduler_ls()
taskscheduleR::taskscheduler_delete(basename(rscript))
```

```{r 'Create Automated Stop loss tasks'}


rscript <- "C:/Users/Administrator/Documents/R/Quant/JobsScripts/AutomatedStopLoss.R"
taskscheduleR::taskscheduler_create(basename(rscript),rscript, schedule = "ONCE", starttime = "06:16", rscript_args = "--verbose",debug = T)
tasks <- taskscheduleR::taskcheduler_runnow("AutomatedStopLoss.R")
taskscheduleR::taskscheduler_delete(basename(rscript))

times <- 10:16 %>% purrr::map(function(.x){
  if(nchar(.x)<2) return(stringr::str_replace(.x,"^","0")) else {
    return(.x)
    }}) %>% unlist %>% paste0(":30")
days  <- c("MON", "TUE",
  "WED", "THU", "FRI")
purrr::map(times, days = days, rscript = rscript, function(.x, days, rscript){
  out <- purrr::map(days, time = .x, rscript = rscript, function(.x, time, rscript){
    taskname = paste0(basename(rscript), "_",.x,"_",stringr::str_replace(time,"\\:",""))
    print(taskname)
    desc <- taskscheduleR::taskscheduler_create(taskname, rscript,
  schedule = "WEEKLY", starttime = time,
  startdate = format(Sys.Date(), "%m/%d/%Y"), rscript_args = "--verbose", days = .x, debug = T)
    return(c(Task = taskname, Desc = desc))
  })
  return(out)
}) %>% purrr::map(function(.x)do.call(rbind,.x)) %>% do.call(rbind,.) %>% assign(x = paste0(basename(rscript)), ., envir = globalenv())
save(list = paste0(basename(rscript)), file = paste0(basename(rscript),".Rdata"))

load(file = "~/R/Quant/AutomatedStopLoss.R.Rdata")
purrr::map(AutomatedStopLoss.R[,"Task"],taskscheduleR::taskscheduler_delete)
load(file = "~/R/Quant/AutomatedOOTDataTest.R.Rdata")
purrr::map(AutomatedOOTDataTest.R[,1],taskscheduleR::taskscheduler_delete)
tasks <- taskscheduleR::taskscheduler_ls()
```





<h3>Arguments</h3>
  <table class="R argblock">
    <tbody>
      <tr>
        <td>
          <code>cl</code>
          <td>
            <p>The column with the closing price. Defaults to 'close' <em>Required</em></p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>buy</code>
          <td>
            <p>The column that indicates when a buy occurs. <em>Required</em></p>
          </td>
        </td>
      </tr>
      <tr>
      <tr>
        <td>
          <code>dcol</code>
          <td>
            <p>Column with lagged differences. <em>Required</em></p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>amt <em>Numeric</em></code>
          <td>
            <p>An amount at which the trailing stop will be set</p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>wind <em>Numeric</em></code>
          <td>
            <p> A window of previous periods for which the average difference between close and lo will be calculated, converted to % of bid, and set as the trailing stop.</p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>std <em>Logical</em></code>
          <td>
            <p> Instead of computing the average of the high to low range for the window as the percent trailing stop, calculate the standard deviation of price over a given window, divide by current bid, and use that as the percent.</p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>prcnt (.dd) format<em>Numeric</em></code>
          <td>
            <p>A fixed decimal percent of the bid price for which the stoploss will be calculated.</p>
          </td>
        </td>
      </tr>
      <tr>
        <td>
          <code>lo</code>
          <td>
            <p>The column with the period low price. Defaults to 'low'</p>
          </td>
        </td>
      </tr>
    </tbody>
  </table>  
```{r 'Add Diff column'}

trailStop <- function(cl,buy,dcol,amt=NULL,wind=NULL,std=NULL,prcnt=NULL,hi=NULL,lo=NULL,verbose=F) {
  trail.args <- c(amt=amt,wind=wind,prcnt=prcnt)
  e <- new.env()
  if(all(is.null(trail.args))){stop("One type of value must be specified")}
  if(exists("wind") & magrittr::not(exists("lo"))){stop("Period low column must be specified")}
  trail.type <- names(trail.args)[which(!is.null(trail.args))]
  vout <- sapply(seq_along(buy),simplify=T,buy=buy,trail.type=trail.type,trail.args=trail.args,cl=cl,dcol=dcol,std=std,lo=lo,hi=hi,FUN=function(i,buy,trail.type,trail.args,cl,lo,hi,dcol,std){
    if(buy[i] == "buy" & trail.type == "wind" & is.null(std) & !is.null(hi)){
      hi.ind <- hi[c(seq(i,i - trail.args[trail.type]))]
      lo.ind <- lo[c(seq(i,i - trail.args[trail.type]))]
      trail <- mean(hi.ind-lo.ind)/cl[i]
      attr(trail,"type") <- "prcnt" # assign the type of the trailing stop
      attr(trail,"i") <- i # assign the pt at which the trailing stop was assigned for creating a cumsum of losses
      assign("trail", trail, envir=e)
    }else if(buy[i] == "buy" & trail.type == "wind" & is.null(std) & is.null(hi)){
      cl.ind <- cl[c(seq(i,i - trail.args[trail.type]))]
      lo.ind <- lo[c(seq(i,i - trail.args[trail.type]))]
      trail <- mean(cl.ind-lo.ind)/cl[i]
      attr(trail,"type") <- "prcnt" # assign the type of the trailing stop
      attr(trail,"i") <- i # assign the pt at which the trailing stop was assigned for creating a cumsum of losses
      assign("trail", trail, envir=e)
    }else if(buy[i] == "buy" & trail.type == "wind" & !is.null(std)){
      cl.ind <- cl[c(seq(i,i - trail.args[trail.type]))]
      trail <- stats::sd(cl.ind)/cl[i]
      attr(trail,"type") <- "prcnt" # assign the type of the trailing stop
      attr(trail,"i") <- i
      assign("trail", trail, envir=e)
      }else if(buy[i] == "buy" & trail.type == "prcnt"){
        trail <- prcnt
        attr(trail,"type") <- "prcnt"
        attr(trail,"i") <- i
        assign("trail", trail, envir=e)
      }else if(buy[i] == "buy" & trail.type == "amt"){
          trail <- amt
          attr(trail,"type") <- "amt"
          attr(trail,"i") <- i
          assign("trail", trail, envir=e)
      }else if(buy[i] == "sell"){
        rm("trail",envir=e)
      }
    out <- "none"
   if(verbose==T){print(i)}
    if(exists("trail",envir=e)){
      trail <- e$trail
      y <- attr(trail,"i")
      cumu <- sum(dcol[seq(i,y)])
    if(verbose==T){print(e$trail)}
    
    if(attr(trail,"type") == "prcnt"){ # Checking Statements
        
         if(cumu < trail*cl[{i-1}]*-1){
           out <- "sell"
           rm("trail",envir=e)}
      }else if(attr(trail,"type") == "amt"){
          if(cumu < trail * -1){
            out <- "sell"
            rm("trail",envir=e)}
      }else {
          out <- "none"
      }
    }else {
          return("none")}
    return(out)
    
    # ----------------------- Thu Apr 26 08:52:37 2018 ------------------------#
    # Need a check on each increment - will need if statement for prcnt type trails (wind,prcnt) and fixed amt
    # Completed(2018-04-26 0913)
  })
  return(vout)
}
#trailStop(cl=TS.adxsar$GOOG$close,buy=TS.adxsar$GOOG$ADXSAR.i,dcol=TS.adxsar$GOOG$Diff,wind=5,lo=TS.adxsar$GOOG$low)
TS.adxsar %<>% lapply(FUN=function(l){
  l %<>% dplyr::mutate(Diff=c(0,diff(close)),Trail=trailStop(cl=close,buy=ADXSAR.i,dcol=Diff,wind=8,std=T))
})

```

<p>Let's step up the complexity a bit with a machine learning algorithm that takes a lag window as inputs and outputs predictions for price movement.</p>

```{r}
library(caret)
library(caretEnsemble)

data.tr <- dplyr::select(TS.adxsar$GOOG,close,sar,ADXv,DIn,DIp,DX,sar.i,adx.i,pr) %>% dplyr::mutate(Diff=c(diff(close),0)) %>% na.omit # make the diff of the next day in line with the current day bc that is what needs to be predicted
data.tr %<>% renamena()
data.tr %<>% convert2dummyVars() 

rv <- "Diff" # Name of response variable
req.packages <- c("caret","doParallel","iterators","parallel","foreach") # [package dependencies]
  startPkgs <- Vectorize(FUN=function(pkg){suppressPackageStartupMessages(library(pkg,character.only = T))})
  HDA::startPkgs(req.packages)
cl <- parallel::makeCluster(parallel::detectCores()-1)
  doParallel::registerDoParallel(cl)
  foreach::getDoParWorkers()
  sysT.mod <- system.time({ #[model name]
  data.train <- caret::createDataPartition(data.tr[[rv]], times = 1, p=.85) #[Data] & [y-var]
  data.train <- caret::trainControl(method="repeatedcv",
                             index=data.train, 
                             number=10,
                             repeats=1, 
                             search = "grid",
                             allowParallel = T,
                             returnData = F,
                             returnResamp = "all")
  form <- stats::as.formula(paste0(rv," ~ .")) #[y-var]
  
  mod <- caret::train(form = form, # [model name]
                      data = data.tr,
                      trControl = data.train,
                      metric="RMSE",
                      method = "avNNet",
                      tuneLength = 10)
  })
 
  parallel::stopCluster(cl);foreach::registerDoSEQ()
```

```{r, 'Predict Diff using Various ML Algorithms', echo=TRUE}
library(caret)
library(caretEnsemble)
data.tr <- renamena(TS.adxsar$GOOG)
data.tr <- dplyr::select(data.tr,close,sar,ADXv,DIn,DIp,DX,sar.i,adx.i,pr) %>% dplyr::mutate(Diff=c(diff(close),0)) %>% na.omit # make the diff of the next day in line with the current day bc that is what needs to be predicted
data.tr[,sapply(data.tr,is.factor)] %<>% lapply(as.numeric)
rv <- "Diff" # Name of response variable
req.packages <- c("caret","doParallel","iterators","parallel","foreach") # [package dependencies]
  startPkgs <- Vectorize(FUN=function(pkg){suppressPackageStartupMessages(library(pkg,character.only = T))})
  HDA::startPkgs(req.packages)
cl <- parallel::makeCluster(parallel::detectCores()-1)
  doParallel::registerDoParallel(cl)
  foreach::getDoParWorkers()
  sysT.mod <- system.time({ #[model name]
  data.train <- caret::createDataPartition(data.tr[[rv]], times = 1, p=.85) #[Data] & [y-var]
  data.train <- caret::trainControl(method="repeatedcv",
                             index=data.train, 
                             number=10,
                             repeats=1, 
                             search = "grid",
                             allowParallel = T,
                             returnData = F,
                             classProbs=T, 
                             savePredictions = "final",
                             returnResamp = "final")
  form <- stats::as.formula(paste0(rv," ~ .")) #[y-var]
  
  mod <- caretEnsemble::caretList(form = form, # [model name]
                                       data = data.tr,
                                       trControl = data.train,
                                       metric="RMSE",
                                       methodList = c("avNNet","gamSpline","kknn"), #[method list]
                                       tuneList = list("avNNEt"=caretEnsemble::caretModelSpec(
                                         method="avNNet", tuneLength = 10),"gamSpline"=caretEnsemble::caretModelSpec(
                                         method="gamSpline", tuneLength = 10),"kknn"=caretEnsemble::caretModelSpec(
                                         method="kknn", tuneLength = 10)))
  parallel::stopCluster(cl);foreach::registerDoSEQ()
  })
  
lapply(mod,purrr::pluck,"results","MAE") %>% lapply(mean)
# avNNet performes the best
caret::confusionMatrix(sign(mod[["gamSpline"]][["pred"]]$pred) %>% as.factor,sign(mod[["gamSpline"]][["pred"]]$obs) %>% as.factor)
mod
```

```{r 'Try Classification'}
diff2fac <- function(Diff){
  if(is.numeric(Diff)){x <- generics::as.factor(sign(Diff))}else{
Diff %<>% forcats::fct_recode('n'='-1','z'='0','p'='1')}}
data.tr %<>% dplyr::mutate(sDiff=diff2fac(sDiff)) 
rv <- "sDiff"
cl <- parallel::makeCluster(parallel::detectCores()-1)
  doParallel::registerDoParallel(cl)
  foreach::getDoParWorkers()
  sysT.mod <- system.time({ #[model name]
  data.train <- caret::createDataPartition(data.tr[[rv]], times = 1, p=.85) #[Data] & [y-var]
  data.train <- caret::trainControl(method="repeatedcv",
                             index=data.train, 
                             number=10,
                             repeats=1, 
                             search = "grid",
                             allowParallel = T,
                             returnData = F,
                             classProbs=T, 
                             savePredictions = "final",
                             returnResamp = "final")
  form <- stats::as.formula(paste0(rv," ~ .")) #[y-var]
  
  mod <- caretEnsemble::caretList(form = form, # [model name]
                                       data = data.tr,
                                       trControl = data.train,
                                       metric="Accuracy",
                                       methodList = c("avNNet","svmRadial","svmLinear","kknn","C5.0","LogitBoost"), #[method list]
                                       tuneList = list("avNNEt"=caretEnsemble::caretModelSpec(
                                         method="avNNet", tuneLength = 10),"svmRadial"=caretEnsemble::caretModelSpec(
                                         method="svmRadial", tuneLength = 10),"svmLinear"=caretEnsemble::caretModelSpec(
                                         method="svmLinear", tuneLength = 10),
    "kknn"=caretEnsemble::caretModelSpec(
                                         method="kknn", tuneLength = 10),
    "C5.0"=caretEnsemble::caretModelSpec(
                                         method="C5.0", tuneLength = 10),
    "LogitBoost"=caretEnsemble::caretModelSpec(
                                         method="LogitBoost", tuneLength = 10)
    ))
  parallel::stopCluster(cl);foreach::registerDoSEQ()
  })
  lapply(mod,purrr::pluck,"results","Accuracy") %>% lapply(mean)
  unloadPkgs <- Vectorize(FUN=function(pkg){detach(pkg,character.only = T)})
  HDA::unloadPkgs(req.packages)
```
<p>Which stocks to invest in, how much to allocate to these stocks, which stocks are trending and which are oscillating, where to set your trailing stop-loss, e.t.c.</p> <a href="https://www.quora.com/Can-machine-learning-algorithms-models-predict-the-stock-prices-If-yes-which-are-the-best-machine-learning-algorithm-models-to-predict-the-stock-prices">Source</a>

<h3>Performance detection</h3>
<p>RV is sign of the cumsum of next day hourly</p>
<ol>
  <li>Basic LM Time ~ Close - slope indicates trend</li>
  <li>Add indicators implied in article + all indicators already functionalized</li>
  <li>Create derived for DIn, DIp subtract(DIp,DIn)</li>
  <li>Moving averages across 4 durations each: day, week, 28 days, quarter</li>
</ol>
<h3>Knn for selection</h3>
<ol>
  <li>Vector of symbol names</li>
  <li>Pull data for each into nested list object</li>
  <li>Add indicators</li>
  <li>Normalize all</li>
  <li>Take 5 most recent rows (head) and rbind</li>
  <li>Select and remove one symbol and data, and run knn prediction on the rest to find 5 predictions for which is the nearest neighbors. </li>
  <li>Take the tie break vote for which stock is performing similarly </li>
</ol>
<hr>
<p>First step, create response variables of varying time windows for use in predicting validity of trading signals.</p>
<table>
  <tbody>
    <tr valign="top">
      <td><code>rv</code></td>
      <td>as a character vector
        <p><em>Required</em> The name of the response variable in the dataset for which windowed gains/losses are to be computed <em>as a character vector</em></p>
      </td>
    <tr valign="top">
      <td><code>wind</code></td>
      <td>
        <p><em>Required</em> The number of periods in the window for which the gain/loss will be computed, <em>can be a positive integer or a vector of postive integers</em></p>
      </td>
    </tr>
    <tr valign="top">
      <td><code>dat</code></td>
      <td>
        <p><em>Required</em> The dataset as a data frame or tibble on which these windowed gains/losses will be computed</p>
      </td>
    </tr>
  </tbody>
</table>





# KNN Selector
```{r 'knn to select similar stocks'}
# ----------------------- Thu Apr 26 18:30:53 2018 ------------------------#
# 

```
```{r 'Add ATR as Pct for Comparison'}
##
```

```{r 'Windowed Predictor Function'}
# ----------------------- Thu Apr 26 18:30:05 2018 ------------------------#
# Stalled till we find a combination of predictors and algorithms with some degree o accuracy

library(caret)
library(caretEnsemble)

windowedmL <- function(df,wind=100,rv="Diff",...){ # df is data, wind= window, rv =  response variable name (character), ... = vectors of indicator columns (name as used in mutate)
  alli <- 101:nrow(df)
  data.tr <- dplyr::select(df,...)
  cl <- parallel::makeCluster(parallel::detectCores()-1)
  doParallel::registerDoParallel(cl)
  foreach::getDoParWorkers()
  sapply(alli,data.tr=data.tr,rv=rv,function(i,data.tr,rv){
    ind <- rev(seq(i,i-wind))
    data.tr <- data.tr[ind,]
    req.packages <- c("caret","doParallel","iterators","parallel","foreach") # [package dependencies]
  startPkgs <- Vectorize(FUN=function(pkg){suppressPackageStartupMessages(library(pkg,character.only = T))})
  HDA::startPkgs(req.packages)
  sysT.mod <- system.time({ #[model name]
  data.train <- caret::createDataPartition(data.tr[[rv]], times = 1, p=.85) #[Data] & [y-var]
  data.train <- caret::trainControl(method="repeatedcv",
                             index=data.train, 
                             number=10,
                             repeats=1, 
                             search = "grid",
                             allowParallel = T,
                             returnData = F,
                             summaryFunction=caret::twoClassSummary,
                             classProbs=T, 
                             savePredictions = "final",
                             returnResamp = "final")
  form <- stats::as.formula(paste0("Diff"," ~ .")) #[y-var]
  
  mod <- caretEnsemble::caretList(form = form, # [model name]
                                       data = data.tr,
                                       trControl = data.train,
                                       metric="RMSE",
                                       methodList = c("bstSm","avNNet","gamSpline","kknn"), #[method list]
                                       tuneList = list("bstSm"=caretEnsemble::caretModelSpec( #[method specs]
                                         method="bstSm", tuneLength = 10),"avNNEt"=caretEnsemble::caretModelSpec(
                                         method="avNNet", tuneLength = 10),"gamSpline"=caretEnsemble::caretModelSpec(
                                         method="gamSpline", tuneLength = 10),"kknn"=caretEnsemble::caretModelSpec(
                                         method="kknn", tuneLength = 10)))
  return(list(sysT.mod,mod))
  })

  })
  
  

  parallel::stopCluster(cl):foreach::registerDoSEQ()
  unloadPkgs <- Vectorize(FUN=function(pkg){detach(pkg,character.only = T)})
  HDA::unloadPkgs(req.packages)
  
}
```


```{r 'Graph SAR Indicator',fig.dim=c(10.5,4)}
library(tidyquant)
lapply(seq_along(TS.adxsar),TS=TS.adxsar,FUN=function(i,TS){
  nm <- names(TS)[i]
  TS[[i]] %>% dplyr::filter(time > lubridate::ymd("2017-05-01") & time < lubridate::ymd("2017-05-31")) %>% ggplot2::ggplot(data=.,ggplot2::aes(x = time, y = close)) +
    tidyquant::geom_candlestick(ggplot2::aes(open = open, high = high, low = low, close = close),size=.3) +
    ggplot2::geom_point(ggplot2::aes(y=sar,shape=sar.i),size=.7)+
    ggplot2::geom_vline(data=function(x){x %>% dplyr::filter(Action == "buy")},ggplot2::aes(xintercept=time),color="blue")+
  ggplot2::geom_vline(data=function(x){x %>% dplyr::filter(Action == "sell")},ggplot2::aes(xintercept=time),color="red")+
    ggplot2::labs(title = "GOOG", 
         y = "Closing Price", x = "Time")+
  tidyquant::theme_tq()+
  ggplot2::scale_shape_manual(values=c(buy=24,sell=25,none=20))
  #ggplot2::ggsave(filename=paste0(nm,"ts.pdf"),plot=last_plot(), path="C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 3\\Plots",device = "pdf",width = 10.5/2, height = 4, units = "in")
})
TS.adxsar$GOOG%>% dplyr::filter(time > lubridate::ymd("2017-05-01") & time < lubridate::ymd("2017-05-31")) %>% ggplot2::ggplot(data=.,ggplot2::aes(x = time, y = close)) +
    tidyquant::geom_candlestick(ggplot2::aes(open = open, high = high, low = low, close = close),size=.3) +
    ggplot2::geom_point(ggplot2::aes(y=sar,shape=sar.i),size=.7)+
    ggplot2::geom_vline(data=function(x){x %>% dplyr::filter(Action == "buy")},ggplot2::aes(xintercept=time),color="blue")+
  ggplot2::geom_vline(data=function(x){x %>% dplyr::filter(Action == "sell")},ggplot2::aes(xintercept=time),color="red")+
    ggplot2::labs(title = "GOOG", 
         y = "Closing Price", x = "Time")+
  tidyquant::theme_tq()+
  ggplot2::scale_shape_manual(values=c(buy=24,sell=25,none=20))
```


```{r 'ADX Signal Stockcharts'}
# ----------------------- Tue Apr 24 09:07:39 2018 ------------------------#
# 

ADX <- TS.indicators[["TSLA"]][128,]$ADX;DX <- TS.indicators[["TSLA"]][128,]$DX;SMA <- TS.indicators[["TSLA"]][128,]$SMA28;DIp <- TS.indicators[["TSLA"]][128,]$DIp;DIn <- TS.indicators[["TSLA"]][128,]$DIn;close <- TS.indicators[["TSLA"]][128,]$close
siADX <- function(ADX,DX,DIp,DIn,close,verbose=F){lgl <- rep(F,3)
names(lgl) <- c("ADX","DI","Close")
  if(is.na(DIp)|is.na(SMA)){return(NA)}else if(is.na(ADX)){ADX <- DX}else {ADX}
  if(verbose==T)print(paste(ADX,DX,SMA,DIp,DIn,close,sep=","))
  if(DIp >= DIn){
  lgl[1] <- ADX > 20
  if(verbose==T)print(lgl)# ADX is above 20
  lgl[2] <- DIp > DIn
  if(verbose==T)print(lgl)
  lgl[3] <- close > SMA
  if(verbose==T)print(lgl)
  ind <- which(lgl)
  if(verbose==T)print(ind)
  if(any(names(ind) %in% "Close")){ind[["Close"]] <- ind[["Close"]] + .5}
  out <- sum(ind * .5 )}else{
    lgl[1] <- T
    lgl[2] <- DIp < DIn
  lgl[3] <- close < SMA
  ind <- which(lgl)
  if(any(names(ind) %in% "Close")){ind[["Close"]] <- ind[["Close"]] + .5}
  out <- sum(ind * .5)* -1}
  return(out)
}
siADX(ADX,DX,SMA,DIp,DIn,close,verbose=T)
siL_ADX <- function(ADX,DX,SMA,DIp,DIn,close,verbose=F){lgl <- rep(F,3)
  names(lgl) <- c("ADX","DI","Close")
  if(is.na(DIp)|is.na(SMA)){return(NA)}else if(is.na(ADX)){ADX <- DX}else {ADX}
  out <- DIp >= DIn & ADX > 20 & DIp > DIn & close > SMA
  return(out)
  }
```
```{r}

# Add MACD - more efficient

TS.indicators <- lapply(TS.indicators,FUN=function(l){
  cbind(l,TTR::TDI(l[,c("close")],n=7,multiple=4))
})

TS.indicators <- lapply(TS.indicators,FUN=function(l){
  cbind(l,RSI7=TTR::RSI(l[,c("close")],n=7),RSI14=TTR::RSI(l[,c("close")],n=14))
})


# ----------------------- Thu Apr 19 06:08:57 2018 ------------------------#
# Now we have some solid indicators to work with.
lapply(TS.indicators,FUN=function(l){ggplot2::ggplot(data = l,mapping=ggplot2::aes(x=date))+
  ggplot2::geom_errorbar(ggplot2::aes(ymin=low,ymax=high),color='blue')+
    ggplot2::geom_point(ggplot2::aes(y=`1_10.sar`,color=`1_10.ind`),size=.2,shape=24,alpha=.5)+
    ggplot2::geom_point(ggplot2::aes(y=`2_20.sar`,color=`2_20.ind`),size=.2,shape=8)})
```




```{r 'Apply ADX Indicator'}
TS.indicators %<>%  lapply(FUN=function(l){
  l %<>% rowwise %>% dplyr::mutate(ADX.ind=siADX(ADX,DX,SMA28,DIp,DIn,close))
})
## 4/20/2018 9:32:05 AM
# Create a buy or sell signal based on each indicator. When the come into agreement, master signal column shows buy/sell signal. 
# buy signal - agreement on indicators (possibly also price < previous sell price)
# sell signal - agreement on indicators - weight given to SAR (sensitivity refinement necessary), if sell price * .8 (tax) - fee > 0 then sell and record transactions
# rebuy signal - need to develop specific indicators attuned to price recovery (as longer term indicators with greater lag might not give agreement to buy signal in these instances.) The MACD, PR20, and SAR might be the best for this measure.
#Automatically records transaction in 2nd DF with all columns necessary for tax reporting.
```

```{r 'Develop RSI Indicator'}
# ----------------------- Tue Apr 24 11:26:17 2018 ------------------------#
# Needs refinemnt of factor indicators

siRSI <- function(RSI,SMA28,SMA100,close) {
  #Oversold, Overbought, Bear, Bull,Support,Resistance
  if(is.na(SMA28)){return(NA)}else if(is.na(SMA100)){SMA <- SMA28}else {SMA <- SMA100}
  
  if(close > SMA){out <- "Up"
  if(RSI >= 50){out <- paste0(out,"St")}else if(RSI <= 40 & RSI > 30){out <- paste0(out,"We")}else if(RSI <= 30){out <- paste0(out,"OS")}else if(RSI < 50 & RSI > 40){paste0(out,"Su")}
  }else if(close < SMA){out <- "Do"
  if(RSI >= 50){out <- paste0(out,"St")}else if(RSI <= 40 & RSI > 30){out <- paste0(out,"We")}else if(RSI >= 70){out <- paste0(out,"OB")}else if(RSI < 50 & RSI > 40){paste0(out,"Re")}
  }
  return(out)
}
```

```{r 'Apply RSI Indicator'}
TS.indicators %<>%  lapply(FUN=function(l){
  l %<>% rowwise %>%  dplyr::mutate(RSI.ind=siRSI(RSI=RSI14,SMA28=SMA28,SMA100=SMA100,close=close))
})

```

```{r 'TDI Indicator'}
siTDI <- function(tdi,di){
  out <- ifelse(sign(tdi) > 0 & sign(di) < 0,T,F)
}
```
```{r 'Apply TDI'}
TS.indicators %<>%  lapply(FUN=function(l){
  l %<>% rowwise %>%  dplyr::mutate(TDI.ind=siTDI(tdi=tdi,di=di))
})
```

```{r 'Buy Sell Indicator'}
# ----------------------- Tue Apr 24 14:11:22 2018 ------------------------#
# Not effective, buy threshold is likely too high

siBS <- function(sar.ind1,sar.ind2,adx.ind,rsi.ind,macd.ind,tdi.ind,verbose=F) {lgl <- rep(F,5)

  formals(siBS) %<>% lapply(FUN=function(x){if(is.na(x)){x <- F}})
  
  
  lgl[["v.sar"]] <- ifelse(sar.ind1 == "rise",T,F)
  if(verbose==T)print(lgl)
  lgl[["v.adx"]] <- ifelse(adx.ind > 2,T,F)
  if(verbose==T)print(lgl)
  lgl[["v.rsi"]] <- stringr::str_detect(rsi.ind,"UpSt")
 if(verbose==T)print(lgl)
  lgl[["v.macd"]] <- ifelse(macd.ind > 0,T,F)
  if(verbose==T)print(lgl)
  lgl[["v.tdi"]] <- tdi.ind
  if(verbose==T)print(lgl)
  
  out <- if(sum(lgl,na.rm=T)/5 > .5){out <- "buy"}else if(sar.ind1 == "fall" & sar.ind2 == "fall"){out <- "sell"}else {out <- "none"}
   
  return(out)
}
# ----------------------- Tue Ap,r 24 14:11:42 2018 ------------------------#
# Take 2, just SAR & ADX
siBS.SAR <- function(sar1,sar2,adx.ind){
  if(is.na(sar1) | is.na(sar2) | is.na(adx.ind)){return("sell")}
  if(sar1 == "rise" & sar2 == "rise" & adx.ind > 2){out <- "buy"}else if(sar1 == "fall" & sar2 == "fall"){out <- "sell"}else {out <- "none"}
  return(out)
}


```
```{r 'Apply Buy Sell Indicator'}
TS.indicators %<>%  lapply(FUN=function(l){
  l %<>% rowwise %>%  dplyr::mutate(BS.ind=siBS(sar.ind1=`2_20.ind`,sar.ind2=`1_10.ind`,adx.ind=ADX.ind,rsi.ind=RSI.ind,macd.ind=macd,tdi.ind=TDI.ind,verbose=F))
})
TS.indicators %<>%  lapply(FUN=function(l){
  l %<>% rowwise %>%  dplyr::mutate(BS.ind=siBS.SAR(sar1=`2_20.ind`,sar2=`1_10.ind`,adx.ind=ADX.ind))
})
```

```{r 'Backtest BS Indicator for 2017'}
TS.GOOG <- TS.indicators[["GOOG"]][lubridate::year(TS.indicators[["GOOG"]]$date) == 2017,]
magrittr::subtract(TS.GOOG[TS.GOOG$date == {TS.GOOG$date %>% max},]$close,TS.GOOG[TS.GOOG$date == {TS.GOOG$date %>% min},]$close) # Overall growth during 2017
# which(TS.GOOG$BS.ind == "buy")
# which(TS.GOOG$BS.ind == "buy")[which(diff(which(TS.GOOG$BS.ind == "buy")) > 1)] 
# which(TS.GOOG$BS.ind == "sell")
# which(TS.GOOG$BS.ind == "sell")[which(diff(which(TS.GOOG$BS.ind == "sell")) > 1)]
cps <- rle(TS.GOOG$BS.ind)
ind <- cumsum(cps$lengths)+1
df.Action <- data.frame(Action=ifelse(cps$values=="sell","buy","sell"),Index=ind)
df.Action %<>% dplyr::mutate(Date=TS.GOOG[ind,]$date,Price=TS.GOOG[ind,]$close)
{df.Action[df.Action$Action == "buy",]-df.Action[df.Action$Action == "sell",]} %>% .$Price %>% sum(na.rm = T)
```
Citations
Data provided for free by <a href="https://iextrading.com/developer" target="_blank">IEX</a>. View <a href="https://iextrading.com/api-exhibit-a/" target="_blank">IEXâ€™s Terms of Use</a>.


### Appendix
```{r 'Windowed Response Variable Fn'}
winRV <- function(rv=NULL,wind=7,dat=NULL) {
  if(!is.vector(wind)|!is.double(wind)){stop("Window must be an integer or a vector of integers")}else {len <- 1:{length(wind)}}
  if(is.null(rv)|is.null(dat)){stop("All variables are required!")}
  # ----------------------- Fri May 04 08:54:09 2018 ------------------------#
  # Primary computation
  tl <- length(dat[[rv]]) # Length of the data as a window limiter
  rvmat <- matrix(nrow = tl, ncol = length(wind)*3,data=rep(NA,tl * length(wind)*3))
  for (i in seq_along(wind)) {
    wi <- wind[i]
    for (r in seq_along(dat[[rv]])) {
      if(r + wi > tl){win <- r:tl}else {win <- r:{r+wi}}# Create Window
      
      rvmat[r,{i*3-2}] <- dat[[rv]][max(win)]-dat[[rv]][min(win)]# Find the Difference for the window and add it to the appropriate matrix column
      
      rvmat[r,{i*3-1}] <- sign(rvmat[r,{i*3-2}]) # Find the sign of the difference and add it to the appropriate matrix column
      rvmat[r,{i*3}] <- (max(dat[[rv]][win])-min(dat[[rv]][win])) * ifelse(which.min(dat[[rv]][win]) > which.max(dat[[rv]][win]),-1,1) # Find the max gain/loss possible for that time period and add it to the appropriate matrix column
      
    }
  }
  colnames(rvmat) <- t(sapply(paste0("y.",c("Diff","Sign","MaxGL")),paste,wind,sep="."))
    cbind(dat,rvmat)
           
       
      
  
}
winRV(rv = "close",wind = wind, dat = Positions_ts$GOOG)
```
```{r 'Finam Hourly Data'}
# Pull Data Deprecated 
TS.adxsar <- POS$Symbol %>% as.character %>% sapply(FUN=function(sym){QuantTools::get_finam_data(sym,from=lubridate::mdy("01-01-2015"), to=lubridate::now(), period = "hour")},simplify = F)
# TS.adxsar %<>% lapply(FUN=function(l){
#   l <- l[,c(1:6)]
# }) # Remove ADX
# Add ADX
TS.adxsar %<>%  lapply(FUN=function(l){
  cbind(l,TTR::ADX(l[,c("high","low","close")],n=20)) %>% dplyr::rename(ADXv=ADX)
})

```
```{r 'YTD 2018 Linear Regression Models'}
lms <- lapply(TS.adxsar, function(l){
  if(stringr::str_detect(names(l),"time") == T){
  l <- l %>% dplyr::mutate("date" = lubridate::as_date(time)) %>% dplyr::group_by(date) %>%  dplyr::summarize_all(.funs = mean) %>% dplyr::mutate_if(.predicate = is.numeric, .funs = scale)
  }
  ytd  <- l %>% dplyr::filter(date > lubridate::ymd("2018-01-01"))
  stats::glm(close ~ date, data = ytd, family = "gaussian")
})
lapply(lms,summary) %>% lapply(purrr::pluck("coefficients"))
```
```{r 'Apply Percent Rise'}
Positions_ts %<>% lapply(tp = timeperiods,verbose=F,FUN=function(l,tp,verbose){
  
    
  # ----------------------- Fri Aug 17 16:06:50 2018 ------------------------#
  # For debug
  # tp <- timeperiods
  # l <- Positions_ts$GOOG
  pRises <- sapply(tp,l=l,verbose=verbose,FUN = function(x,l,verbose){
    pRise(l$close,x,verbose)
  })
  l <- cbind(l,pRises)
  colIndexes<- seq({ncol(l)-length(tp)+1},ncol(l))
  colNames <- sapply(tp,FUN = function(x){
    paste0("pRise.", x)
  })
  names(l)[colIndexes] <- colNames
  return(l)
})
```
```{r 'Apply sDxts'}
Positions_ts %<>% lapply(tp = timeperiods,verbose=F,FUN=function(l,tp,verbose){
  # ----------------------- Fri Aug 17 16:06:50 2018 ------------------------#
  # For debug
  # tp <- timeperiods
  # l <- Positions_ts$GOOG
  
  l <- cbind(l,sDs)
  colIndexes<- seq({ncol(l)-length(tp)+1},ncol(l))
  colNames <- sapply(tp,FUN = function(x){
    paste0("sd.", x)
  })
  names(l)[colIndexes] <- colNames
  return(l)
})
```
```{r 'Apply Percent Change'}
Positions_ts %>% lapply(tp = timeperiods,verbose = F,FUN = function(l,tp,verbose){
  # ----------------------- Fri Aug 17 16:06:50 2018 ------------------------#
  # For debug
  # tp <- timeperiods
  # l <- Positions_ts$GOOG
  pChanges <- sapply(tp,l=l,verbose=verbose,FUN = function(x,l,verbose){
    pChange(l$close,x,verbose)
  })
  l <- cbind(l,pChanges)
  colIndexes<- seq({ncol(l)-length(tp)+1},ncol(l))
  colNames <- sapply(tp,FUN = function(x){
    paste0("pChange.", x)
  })
  names(l)[colIndexes] <- colNames
  return(l)
})
```

#Appendix
## Previous method for creating RV's
```{r 'Old RV', eval=F}
# ----------------------- Sun May 19 08:33:01 2019 ------------------------#
# Doesnt work properly, error in findPeaks

list(function(x){.1}, .5, function(x){})
Positions_ts %<>% lapply(wind = wind, verbose = T, function(l, wind, verbose){
  dat <- cbind(quantmod::OHLC(l),l[,"sar_i"])
  cl_nm <- grep("close", names(dat), ignore.case = T)
  sa_nm <- grep("sar", names(dat), ignore.case = T)
  if (all(!is.na(dat))) {
    rvs <- purrr::map(wind, env = parent.frame(), function(.x, env){
      if (nrow(dat) <= .x) .x <- {nrow(dat) - 1}
      ras <- zoo::rollapplyr(dat[,c(cl_nm, sa_nm)], width = .x, align = "left", by.column = F, env = parent.frame(), na.pad = T, partial = T, function(r, env){
        if (!is.na(which(r[, 2] == -1)[1])) s <- which(r[, 2] == -1)[1] else s <- numeric(0)
        if(verbose == T) print(s)
        p <- quantmod::findPeaks(r[, 1], thresh = stats::sd(r[1]))
        v <- quantmod::findValleys(r[, 1], thresh = stats::sd(r[1]))
        if (length(s) != 0) i <- s[1] else if (length(p) != 0) i <- p[1] else if (length(v) != 0) i <- v[1] else i <- .x
        if(verbose == T) print(i)
        out <- {zoo::coredata(r[i, 1]) - zoo::coredata(r[1, 1])}
        if(verbose == T) print(out)
        return(out)
      })
      # Fill the NA at the end
      na_ind <- which(is.na(ras)) 
      n <- min(na_ind)
      end <- max(na_ind)
      while (n < end) {
        # Use the SAR as the Sell point first, Which returns index within n:na_ind, so to get the actual index it needs to be used as a subset of n:end
        if (!is.na(which(dat[n:end, sa_nm] == -1)[1])) s <- c(n:end)[which(dat[n:end, sa_nm] == -1)[1]] else s <- numeric(0)
        p <- c(n:end)[quantmod::findPeaks(dat[n:end, cl_nm], thresh = stats::sd(dat[n:end, cl_nm]))]
        v <- c(n:end)[quantmod::findValleys(dat[n:end, cl_nm], thresh = stats::sd(dat[n:end, cl_nm]))]
        if(verbose == T) print(c(s=s,p=p,v=v))
        if (length(s) != 0) i <- s[1] else if (length(p) != 0) i <- p[1] else if (length(v) != 0) i <- v[1] else i <- end
        if(verbose == T) print(i)
        ras[n] <- {zoo::coredata(dat[i, cl_nm]) - zoo::coredata(dat[n, cl_nm])}
        n <- n + 1
      }
      ras[n] <- 0
      # END
      return(ras)
    })
  out <- do.call("cbind", rvs)
  colnames(out) <- paste0(names(out), ".","_rv")
  l <- l[,- grep("rv$",names(l))]
  out <- cbind(l, out)
  }else out <- l
  return(out)
})

```

## ADX & SAR Algorithm from Finance for Freelancers
```{r 'Combine SAR & ADX as Indicator'}
ADXSAR.i <- function(adx.ind=adx.i,sar.ind=sar.i,wind=3) { # Both indicators and a window
  vout <- sapply(seq_along(adx.ind),adx.ind=adx.ind,sar.ind=sar.ind,wind=wind,FUN=function(i,adx.ind,sar.ind,wind){ind <- seq(i,i+wind,by=1)
    if(adx.ind[i] == "buy" & any(sar.ind[ind] %in% "buy")){out <- "buy"}else if(sar.ind[i]=="sell"){out <- "sell"}
  return(out)
  })
  return(vout)
}
# For Debugging
adx.i <- TS.adxsar[["GOOG"]]$adx.i
sar.i <- TS.adxsar[["GOOG"]]$sar.i
```
```{r 'Apply ADXSAR Indicator'}
TS.adxsar  %<>% lapply(FUN=function(l){
  if(any(names(l) %in% "ADXSAR.i")){l %<>% dplyr::select(- ADXSAR.i)}
  l %>% dplyr::mutate(ADXSAR.i=ADXSAR.i(sar.ind=sar.i,adx.ind=adx.i))
})
```
```{r 'Compute Buy and Sell Points'}
# ----------------------- Tue Apr 24 19:10:12 2018 ------------------------#
# ADX needs built in stoploss with diff

TS.adxsar %<>% lapply(FUN=function(l){

bs.rle <- l$ADXSAR.i %>% rle
ind <- cumsum(bs.rle$lengths)+1
ind[length(ind)] <- ind[length(ind)]-1
b.ind <- which(bs.rle$values == "sell") # The buy indexes come when the rle value changes, so a buy is indicated by sell
s.ind <- which(bs.rle$values == "buy") # and vice versa
l %<>% dplyr::mutate(Action=NA)
l$Action[ind[s.ind]] <- rep("sell",length(s.ind))
l$Action[ind[b.ind]] <- rep("buy",length(b.ind))

# df <- data.frame(action=c(rep("sell",length(s.ind)),rep("buy",length(b.ind))),indices=c(ind[s.ind],ind[b.ind])) 
# df$time <- .y$time[df$indices]
# df$close <- .y$close[df$indices]
# df.Action <- data.frame(Action=ifelse(cps$values=="sell","buy","sell"),Index=ind)
# df.Action %<>% mutate(Date=TS.GOOG[ind,]$date,Price=TS.GOOG[ind,]$close)
# {df.Action[df.Action$Action == "buy",]-df.Action[df.Action$Action == "sell",]} %>% .$Price %>% sum(na.rm = T)

return(l)
})
```

```{r 'Compare to Buy and Hold'}
{TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "sell","close"] - TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "buy","close"]} %>% sum(na.rm = T)
{TS.adxsar.signals$GOOG[which.max(TS.adxsar.signals$GOOG$time),"close"] - TS.adxsar.signals$GOOG[which.min(TS.adxsar.signals$GOOG$time),"close"]}
purrr::map2(.x = TS.adxsar, T)
# ----------------------- Tue Apr 24 23:10:50 2018 ------------------------#
# Close - still no better than buy and hold. Needs a trailing stop mechanism. Assign % stop loss to window mean(close[i:i-6]-low[i:i-6])/close
```
```{r 'Patterns in ADX SAR Algo'}

# ----------------------- Thu Apr 26 11:36:55 2018 ------------------------#
# Join the buy points with the loss or gain per period and see if there are patterns
{TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "sell","close"] - TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "buy","close"]} %>% length
TS.adxsar$GOOG %>% dplyr::filter(Action=="buy") %>% dplyr::select(time,PR,sar,ADXv,DX,Diff,sar.i) %>% cbind(G.L={TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "sell","close"] - TS.adxsar.signals$GOOG[TS.adxsar.signals$GOOG$action == "buy","close"]}) %>% stats::lm(G.L ~ ., data=.) %>% summary

```


<p>The response variables of interest currently are:
<ul>
<li> the percent change for varying periods of time </li>
<li> the percent of days of the time period in which gains were recognized</li>
<li> the sd for the same time periods </li>
</ul>
</p>

```{r 'Input RV Time Periods'}
# ----------------------- Fri Aug 17 15:23:09 2018 ------------------------#
# timeperiods can be in Date: duration format, or number of intra-day observations. ie lubridate::duration(1,"weeks") or lubridate::dweeks(1). 
timeperiods <- c(lubridate::dweeks(1),lubridate::dweeks(2),lubridate::duration(1,"months"))
```

```{r 'Percent Change'}
addpChange <- function(d.f,varnm = "close", wind = 5, verbose = F){
  clos <- d.f[,varnm]
  v <- verbose
dopC  <- lapply(wind,function(tp,cl = clos, verbose = v){
    vout <- sapply(seq_along(cl),win=tp,.cl=cl,.verb=verbose,FUN=function(i,win,.cl,.verb){
   if(lubridate::is.duration(win)!=T){ 
    if(i-win < 1){ind <- 1}else {ind <- i-win} # Cases where the lag falls outside the time window
    if(.verb==T){print(i);print(c(.cl[ind],.cl[i]))}
   }else if(lubridate::is.duration(win)){
     if(stats::time(.cl)[i] - win < stats::time(.cl)[1]){ind <- 1}else {ind <- which(abs(stats::time(.cl)-(stats::time(.cl)[i] - win)) == min(abs(stats::time(.cl) - (stats::time(.cl)[i] - win))))[1]} # Cases where the lag falls outside the time window
      if(.verb==T){print(stats::time(.cl)[i])}
   }
    out <- (.cl[[ind]]-.cl[[i]])/.cl[[ind]]
    if(is.na(out)){out <- 0}
    return(out)
  })
})

  
 if(lubridate::is.duration(wind)){nm <- paste0("pC",wind %>% stringr::str_extract("(?<=\\(\\~)[\\s[:alnum:]\\.]+"))}else{nm <- paste0("pC",wind)}
nm %<>% make.names
  out <- cbind(d.f,do.call("cbind",dopC))
  names(out)[(dim(out)[2]-(length(dopC)-1)):dim(out)[2]] <- nm
  return(out)
}
# ----------------------- Fri Aug 17 15:53:03 2018 ------------------------#
# For Testing
cl <- Positions_ts$GOOG$close
wind <- timeperiods[1]
verbose <- F
addpChange(Positions_ts[[1]],varnm = "close",wind = timeperiods,verbose=F) %>% as.data.frame

```

```{r 'Standard Deviation'}
sDxts <- function(cl,wind = 5,verbose=F){
  vout <- sapply(seq_along(cl),wind=wind,cl=cl,verbose=verbose,FUN=function(i, wind, cl, verbose){
    if(lubridate::is.duration(wind)!=T){
      if(i-wind < 1){ind <- 1}else {ind <- i-wind}
      if(verbose==T){print(i);print(c(cl[ind],cl[i]))}
    }else if(lubridate::is.duration(wind)){
      if(stats::time(cl)[i] - wind < stats::time(cl)[1]){ind <- 1}else {ind <- which(stats::time(cl)[i] - wind == stats::time(cl))}
      if(verbose==T){stats::time(cl)[i]}
    }
  cl %<>% scale
  out <- stats::sd(cl[c(seq(i,ind))],na.rm = T)
  if(is.na(out)){out <- 0}
return(out)
  })
}
# test <- sd.xts(cl,wind,F)
```

```{r 'Percent Rise'}
pRise <- function(cl,wind,verbose=F){
  vout <- sapply(seq_along(cl),wind=wind,cl=cl,verbose=verbose,FUN=function(i,wind,cl,verbose){
    if(i==1){return(0)} # Account for a single day that will error diff
    if(lubridate::is.duration(wind)!=T){
      if(i-wind < 1){ind <- 1}else {ind <- i-wind}
      if(verbose==T){print(i);print(c(cl[ind],cl[i]))}
    }else if(lubridate::is.duration(wind)){
      if(stats::time(cl)[i] - wind < stats::time(cl)[1]){ind <- 1}else {ind <- which(stats::time(cl)[i] - wind == stats::time(cl))}
      if(verbose==T){stats::time(cl)[i]}
    }
   out <- cl[c(seq(i,ind))] %>% rev() %>% diff() %>% sign() %>% table() %>% prop.table() %>% .["1"]
   if(is.na(out)){return(0)} # Account for no rise
    return(out)
   # Get the values in the window. Because seq is in reverse, the values need to flipped with rev. Take the diff, get the sign of the difference, table that, then give proportions, assign the proportion associated with positive gains.
   
  })
}
#test <- pRise(cl,wind,verbose=F)
```

```{r 'addRV Meta Function'}
# ----------------------- Sat Aug 18 07:44:32 2018 ------------------------#
# This Function accepts an input of the types of RVs and adds them to the list of timeseries rather than running each chunk individually.

test <- lapply(Positions_ts,rv = c("pChange","pRise","sd"), tp = timeperiods, FUN = function(l,rv,tp, verbose = F){
vout <- list()
# ----------------------- Sat Aug 18 07:12:17 2018 ------------------------#
# percent Change
if(any(rv == "pChange")){
vout$pChanges <- sapply(tp,l=l,verbose=verbose,FUN = function(x,l,verbose){
    pChange(l$close,x,verbose)
  })
}
# ----------------------- Sat Aug 18 07:12:02 2018 ------------------------#
# percent Rise
if(any(rv == "pRise")){
vout$pRise <- sapply(tp,l=l,verbose=verbose,FUN = function(x,l,verbose){
    pRise(l$close,x,verbose)
  })
}
# ----------------------- Sat Aug 18 07:13:12 2018 ------------------------#
# Standard Deviation
  if(any(rv == "sd")){
vout$sd <- sapply(tp,l=l,verbose=verbose,FUN = function(x,l,verbose){
    sDxts(l$close,x,verbose)
  })
}
# Label all columns
vout <- lapply(seq_along(vout),tp = tp, nms = names(vout),vout=vout, FUN = function(o,tp,nms,vout){
  if(lubridate::is.duration(tp)){tp <- timeperiods %>% as.character() %>% stringr::str_extract("((?<=\\~)[\\.A-Za-z0-9\\s]+)") %>% gsub(" ","",.)}
  colNames <- sapply(tp,nm = nms[o],FUN = function(x,nm){
    paste0(nm,".", x)
  })
  colnames(vout[[o]]) <- colNames
  return(vout[[o]])
})
rvs <- do.call("cbind",vout)
l <- cbind(l,rvs)
})
```


## Add Simple Moving Averages
<p>The function below adds moving averages for 7 hour periods (1 day), 35 hr periods (1 week), 140 hr periods (one moon cycle), and 420 hr periods (a financial quarter). All of these values are obviously rough estimates as they are absolute periods and do not take into account holidays.</p>
```{r 'Add SMA Iterations'}
TS.adxsar %>% lapply(ns=c(dy=7,wk=35,mc=140,qu=420),FUN=function(l,ns){
  smas <- sapply(ns,close=l[,c("close")],FUN=function(ni,close){
    TTR::SMA(close,n=ni)
  })
  colnames(smas) <- paste("SMA",ns,sep=".")
  cbind(l,smas)
})
```
## Costbasis used for 2018 Taxes
```{r 'Calculating Cost Basis'}
CostBasis18 <- utils::read.csv("~/R/Quant/PortfolioCostBasis2018-07-09.csv")
# ----------------------- Mon Jul 09 19:09:54 2018 ------------------------#
# Clean Data
sapply(CostBasis18, class)
CostBasis18[, c("Date.Sold","Date.Acquired")] %<>% lapply(FUN=lubridate::dmy)
CostBasis18 %<>% dplyr::rename(`Gain.Loss.Pct` = `Gain.Loss..`)
CostBasis18_Net[, c("Proceeds","Cost.per.Share","Cost","Gain.Loss","Gain.Loss.Pct")] %<>% lapply(.,FUN=function(x)gsub("\\$|\\,|\\%","",x)) %>% lapply(as.numeric)
# ----------------------- Mon Jul 09 20:01:34 2018 ------------------------#
# Portfolio Returns YTD 2018
CostBasis18 %>% dplyr::group_by(Symbol) %>% dplyr::summarise_at(.vars = c("Quantity","Cost","Proceeds","Cost.per.Share","Gain.Loss","Gain.Loss.Pct"), .funs = sum) %<>% dplyr::mutate(Wgt = {Cost/sum(CostBasis18$Cost)}) %>% dplyr::mutate(W.Yld = Wgt * Gain.Loss.Pct)
```
